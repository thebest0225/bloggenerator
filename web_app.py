from flask import Flask, render_template, request, jsonify, session, send_file, send_from_directory, redirect, url_for
from flask_cors import CORS
import urllib.request
import urllib.parse
import urllib.error
import json
import re
import time
import threading
import os
import sys
import tempfile
import shutil
from datetime import datetime
import uuid

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import requests
    from pytrends.request import TrendReq
    TRENDS_AVAILABLE = True
except ImportError:
    TRENDS_AVAILABLE = False
    print("âš ï¸ pytrendsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install pytrends requests' ì‹¤í–‰í•˜ì„¸ìš”.")

try:
    import prompts
    PROMPTS_AVAILABLE = True
except ImportError:
    PROMPTS_AVAILABLE = False

app = Flask(__name__)
app.secret_key = 'your-secret-key-here'  # ì‹¤ì œ ë°°í¬ì‹œ ë³€ê²½ í•„ìš”
app.config['SESSION_TYPE'] = 'filesystem'
CORS(app)

# íŒ¨ìŠ¤ì›Œë“œ ì„¤ì •
ADMIN_PASSWORD = "0225"

class BlogWebApp:
    def __init__(self):
        self.client_id = None
        self.client_secret = None
        self.openai_api_key = None
        self.load_env_variables()

        # ì„ì‹œ ê²°ê³¼ ì €ì¥ìš© (ì‹¤ì œ ë°°í¬ì‹œì—ëŠ” Redisë‚˜ DB ì‚¬ìš© ê¶Œì¥)
        self.temp_results = {}

    def load_env_variables(self):
        """í™˜ê²½ë³€ìˆ˜ ë¡œë“œ"""
        print("ğŸ”§ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì‹œì‘...")
        
        # ë¨¼ì € ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸° (Replit Secrets)
        self.client_id = os.environ.get('NAVER_CLIENT_ID')
        self.client_secret = os.environ.get('NAVER_CLIENT_SECRET_KEY')
        self.openai_api_key = os.environ.get('OPENAI_API_KEY')

        print(f"ğŸ“‹ ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ í™•ì¸:")
        print(f"  - NAVER_CLIENT_ID: {'ìˆìŒ (' + self.client_id[:8] + '...)' if self.client_id else 'ì—†ìŒ'}")
        print(f"  - NAVER_CLIENT_SECRET_KEY: {'ìˆìŒ (' + self.client_secret[:8] + '...)' if self.client_secret else 'ì—†ìŒ'}")
        print(f"  - OPENAI_API_KEY: {'ìˆìŒ (' + self.openai_api_key[:8] + '...)' if self.openai_api_key else 'ì—†ìŒ'}")

        # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ .env íŒŒì¼ì—ì„œ ì‹œë„
        if not all([self.client_id, self.client_secret, self.openai_api_key]):
            env_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.env')
            print(f"ğŸ“ .env íŒŒì¼ ê²½ë¡œ: {env_file_path}")

            try:
                with open(env_file_path, 'r', encoding='utf-8') as f:
                    print("ğŸ“„ .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì¤‘...")
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#') and '=' in line:
                            key, value = line.split('=', 1)
                            key = key.strip()
                            value = value.strip()

                            if key == 'NAVER_CLIENT_ID' and not self.client_id:
                                self.client_id = value
                                print(f"  âœ“ NAVER_CLIENT_ID ë¡œë“œë¨")
                            elif key == 'NAVER_CLIENT_SECRET_KEY' and not self.client_secret:
                                self.client_secret = value
                                print(f"  âœ“ NAVER_CLIENT_SECRET_KEY ë¡œë“œë¨")
                            elif key == 'OPENAI_API_KEY' and not self.openai_api_key:
                                self.openai_api_key = value
                                print(f"  âœ“ OPENAI_API_KEY ë¡œë“œë¨")
            except FileNotFoundError:
                print(f"âŒ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {env_file_path}")
                print("ğŸ’¡ Replit Secretsì— ë‹¤ìŒ í‚¤ë“¤ì„ ì„¤ì •í•´ì£¼ì„¸ìš”:")
                print("   - NAVER_CLIENT_ID")
                print("   - NAVER_CLIENT_SECRET_KEY") 
                print("   - OPENAI_API_KEY")

        # ìµœì¢… ë¡œë“œëœ í™˜ê²½ë³€ìˆ˜ ìƒíƒœ ì¶œë ¥
        print(f"ğŸ” ìµœì¢… í™˜ê²½ë³€ìˆ˜ ìƒíƒœ:")
        print(f"  - NAVER_CLIENT_ID: {'âœ… ì„¤ì •ë¨' if self.client_id else 'âŒ ì„¤ì • í•„ìš”'}")
        print(f"  - NAVER_CLIENT_SECRET_KEY: {'âœ… ì„¤ì •ë¨' if self.client_secret else 'âŒ ì„¤ì • í•„ìš”'}")
        print(f"  - OPENAI_API_KEY: {'âœ… ì„¤ì •ë¨' if self.openai_api_key else 'âŒ ì„¤ì • í•„ìš”'}")
        
        if not self.client_id or not self.client_secret:
            print("âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        if not self.openai_api_key:
            print("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AI ë¶„ì„ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    def search_naver_blog(self, query, display=50, sort='date'):
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
        if not self.client_id or not self.client_secret:
            raise Exception("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Secretsì—ì„œ NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRET_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

        print(f"ğŸ” ë„¤ì´ë²„ API í˜¸ì¶œ ì‹œì‘: {query}")
        print(f"ğŸ“Š Client ID: {self.client_id[:8]}...")
        print(f"ğŸ” Client Secret: {self.client_secret[:8]}...")

        enc_text = urllib.parse.quote(query)
        url = f"https://openapi.naver.com/v1/search/blog.json?query={enc_text}&display={display}&sort={sort}"

        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", str(self.client_id))
        request.add_header("X-Naver-Client-Secret", str(self.client_secret))
        request.add_header("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")

        try:
            print(f"ğŸŒ API URL: {url}")
            response = urllib.request.urlopen(request, timeout=30)
            if response.getcode() == 200:
                response_body = response.read()
                result = json.loads(response_body.decode('utf-8'))
                print(f"âœ… API í˜¸ì¶œ ì„±ê³µ: {result.get('total', 0)}ê±´ ê²€ìƒ‰ë¨")
                return result
            else:
                error_body = response.read().decode('utf-8') if response else "ì‘ë‹µ ì—†ìŒ"
                print(f"âŒ API ì‘ë‹µ ì˜¤ë¥˜: HTTP {response.getcode()}")
                print(f"ğŸ“ ì˜¤ë¥˜ ë‚´ìš©: {error_body}")
                raise Exception(f"ë„¤ì´ë²„ API ì‘ë‹µ ì˜¤ë¥˜: HTTP {response.getcode()}")
        except urllib.error.HTTPError as e:
            error_body = e.read().decode('utf-8') if hasattr(e, 'read') else str(e)
            print(f"âŒ HTTP ì˜¤ë¥˜: {e.code} - {error_body}")
            if e.code == 401:
                raise Exception("ë„¤ì´ë²„ API ì¸ì¦ ì‹¤íŒ¨. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            elif e.code == 400:
                raise Exception("ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                raise Exception(f"ë„¤ì´ë²„ API ì˜¤ë¥˜ (HTTP {e.code}): {error_body}")
        except urllib.error.URLError as e:
            print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜: {e}")
            raise Exception(f"ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            raise Exception(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    def clean_html_tags(self, text):
        """HTML íƒœê·¸ ì œê±°"""
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)

    def extract_blog_data(self, search_result):
        """ë¸”ë¡œê·¸ ë°ì´í„° ì¶”ì¶œ"""
        if not search_result or 'items' not in search_result:
            return [], []

        titles = []
        descriptions = []

        for item in search_result['items']:
            title = self.clean_html_tags(item.get('title', ''))
            description = self.clean_html_tags(item.get('description', ''))

            if title:
                titles.append(title)
                descriptions.append(description)

        return titles, descriptions

    def analyze_with_gpt(self, titles, descriptions, query, analysis_type='comprehensive'):
        """GPTë¡œ ë¸”ë¡œê·¸ ì œëª©ê³¼ ë³¸ë¬¸ ë‚´ìš© ì¢…í•© ë¶„ì„"""
        if not OPENAI_AVAILABLE:
            raise Exception("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if not self.openai_api_key:
            raise Exception("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        client = OpenAI(api_key=self.openai_api_key)

        print(f"ğŸ” ë¶„ì„ ì‹œì‘: ì œëª© {len(titles)}ê°œ, ë³¸ë¬¸ {len(descriptions)}ê°œ")
        print(f"ğŸ“Š ë¶„ì„ ìœ í˜•: {analysis_type}")

        if PROMPTS_AVAILABLE:
            try:
                analysis_types = prompts.get_available_analysis_types()
                analysis_key = None
                for key, config in analysis_types.items():
                    if config['name'] == analysis_type:
                        analysis_key = key
                        break

                if not analysis_key:
                    analysis_key = 'comprehensive'

                system_prompt = prompts.get_system_prompt(analysis_key)
                user_prompt = prompts.create_content_analysis_prompt(query, titles, descriptions, analysis_key)
                print("âœ… prompts.pyì—ì„œ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ prompts.py ì‚¬ìš© ì¤‘ ì˜¤ë¥˜: {e}")
                system_prompt = "ë‹¹ì‹ ì€ ë¸”ë¡œê·¸ ì½˜í…ì¸  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                user_prompt = self.create_fallback_content_analysis_prompt(query, titles, descriptions)
        else:
            print("âŒ prompts.py ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            system_prompt = "ë‹¹ì‹ ì€ ë¸”ë¡œê·¸ ì½˜í…ì¸  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
            user_prompt = self.create_fallback_content_analysis_prompt(query, titles, descriptions)

        print("ğŸš€ OpenAI API í˜¸ì¶œ ì‹œì‘ (ì œëª© + ë³¸ë¬¸ ì¢…í•© ë¶„ì„)")

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=4000,
            temperature=0.7
        )

        analysis_result = response.choices[0].message.content
        print(f"âœ… ì œëª© + ë³¸ë¬¸ ì¢…í•© ë¶„ì„ ì™„ë£Œ ({len(analysis_result)}ì)")

        return analysis_result

    def create_fallback_content_analysis_prompt(self, query, titles, descriptions):
        """í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆì´ ì—†ì„ ë•Œ ì‚¬ìš©í•  ê¸°ë³¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸ - ì œëª©ê³¼ ë³¸ë¬¸ ì¢…í•© ë¶„ì„"""
        content_list = []
        for i, (title, desc) in enumerate(zip(titles, descriptions)):
            # ë³¸ë¬¸ ë‚´ìš©ì„ ë” ë§ì´ í¬í•¨í•˜ì—¬ ë¶„ì„ì˜ ì •í™•ë„ í–¥ìƒ
            desc_preview = desc[:500] if desc else "ë³¸ë¬¸ ë‚´ìš© ì—†ìŒ"
            content_list.append(f"{i+1}. ì œëª©: {title}\n   ë³¸ë¬¸ ë‚´ìš©: {desc_preview}{'...' if len(desc) > 500 else ''}")

        return f"""ë‹¤ìŒì€ '{query}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ ì œëª©ê³¼ ë³¸ë¬¸ ë‚´ìš©ì…ë‹ˆë‹¤. 
ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ê¹Šì´ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

=== ë¸”ë¡œê·¸ ì½˜í…ì¸  ëª©ë¡ ({len(titles)}ê°œ) ===
{chr(10).join(content_list)}

=== ì œëª© + ë³¸ë¬¸ ì¢…í•© ë¶„ì„ ìš”ì²­ ===

## 1. ì½˜í…ì¸  íŠ¸ë Œë“œ ë° ë°©í–¥ì„± ë¶„ì„
- í˜„ì¬ '{query}' ê´€ë ¨ ê°€ì¥ ì¸ê¸° ìˆëŠ” ì£¼ì œì™€ ì ‘ê·¼ ë°©ì‹
- ì œëª©ì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ë…ìë“¤ì˜ ì£¼ìš” ê´€ì‹¬ì‚¬
- **ë³¸ë¬¸ ë‚´ìš©ì„ í†µí•´ íŒŒì•…ë˜ëŠ” ì‹¤ì œ ë‹¤ë¤„ì§€ëŠ” ì„¸ë¶€ ì£¼ì œë“¤**
- **ì œëª©ê³¼ ë³¸ë¬¸ì—ì„œ ê³µí†µìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œì™€ íŒ¨í„´**
- ì‹œì˜ì„± ìˆëŠ” í‚¤ì›Œë“œì™€ íŠ¸ë Œë“œ ë³€í™”

## 2. ì½˜í…ì¸  í’ˆì§ˆ ë° ê¹Šì´ ë¶„ì„
- **ì œëª©ê³¼ ë³¸ë¬¸ ë‚´ìš©ì˜ ì¼ì¹˜ë„ ë° ì •í•©ì„± í‰ê°€**
- í‘œë©´ì  ì •ë³´ vs ì‹¬í™” ë‚´ìš©ì˜ ë¹„ìœ¨
- **ë³¸ë¬¸ì—ì„œ ì œê³µí•˜ëŠ” ì‹¤ìš©ì  ì •ë³´ì˜ êµ¬ì²´ì„±ê³¼ ê¹Šì´**
- ë…ìì—ê²Œ ì‹¤ì§ˆì  ë„ì›€ì´ ë˜ëŠ” ì½˜í…ì¸ ì˜ íŠ¹ì§•
- **ë³¸ë¬¸ì—ì„œ ë‹¤ë£¨ëŠ” ì£¼ìš” ë¬¸ì œì ê³¼ í•´ê²°ì±…ì˜ ì§ˆ**

## 3. ì½˜í…ì¸  êµ¬ì¡° ë° ì ‘ê·¼ ë°©ì‹ ë¶„ì„
- íš¨ê³¼ì ì¸ ì œëª© êµ¬ì¡°ì™€ ë³¸ë¬¸ ì „ê°œ íŒ¨í„´
- **ë³¸ë¬¸ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ì •ë³´ ì „ë‹¬ ë°©ì‹ (ê°€ì´ë“œí˜•, ê²½í—˜ë‹´, ë¦¬ë·°, ë¹„êµë¶„ì„ ë“±)**
- **ë³¸ë¬¸ì—ì„œ ë…ì ì°¸ì—¬ë¥¼ ìœ ë„í•˜ëŠ” ìš”ì†Œë“¤ê³¼ ìŠ¤íƒ€ì¼**
- SEO ìµœì í™” ê´€ì ì—ì„œì˜ í‚¤ì›Œë“œ í™œìš© (ì œëª© + ë³¸ë¬¸)
- **ë³¸ë¬¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ í†¤ê³¼ ë…ìì™€ì˜ ì†Œí†µ ë°©ì‹**

## 4. ì‹œì¥ ê¸°íšŒ ë° ì°¨ë³„í™” ì „ëµ
- **í˜„ì¬ ì½˜í…ì¸ ë“¤ì˜ ë³¸ë¬¸ ë¶„ì„ì„ í†µí•œ í•œê³„ì ê³¼ ê°œì„  ê¸°íšŒ**
- **ë³¸ë¬¸ì—ì„œ ë‹¤ë£¨ì§€ ëª»í•˜ê³  ìˆëŠ” ë…ìë“¤ì˜ ê¶ê¸ˆì¦ê³¼ ë‹ˆì¦ˆ**
- ì°¨ë³„í™” ê°€ëŠ¥í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ê³¼ ì½˜í…ì¸  ê°ë„
- **ë³¸ë¬¸ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ê²½ìŸë ¥ ìˆëŠ” ì½˜í…ì¸  ì‘ì„± ì „ëµ**

## 5. ë…ì ë‹ˆì¦ˆ ë° ê²€ìƒ‰ ì˜ë„ ì¢…í•© ë¶„ì„
- **ì œëª©ê³¼ ë³¸ë¬¸ì„ í†µí•´ íŒŒì•…ë˜ëŠ” ê²€ìƒ‰ìë“¤ì˜ ì§„ì§œ ëª©ì ê³¼ ê¸°ëŒ€ì‚¬í•­**
- **ë³¸ë¬¸ì—ì„œ í•´ê²°í•˜ë ¤ëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œë“¤ê³¼ í•´ê²° ìˆ˜ì¤€**
- ì •ë³´ íƒìƒ‰ ë‹¨ê³„ë³„ ë‹ˆì¦ˆ ì°¨ì´ì™€ ë§Œì¡±ë„
- **ë³¸ë¬¸ ë‚´ìš©ìœ¼ë¡œ íŒë‹¨í•œ ë…ìì˜ ì§€ì‹ ìˆ˜ì¤€ê³¼ ê´€ì‹¬ ì˜ì—­**

## 6. ìƒˆë¡œìš´ ë¸”ë¡œê·¸ ê¸€ ì‘ì„±ì„ ìœ„í•œ ì „ëµì  ì œì•ˆ
- **ë¶„ì„ëœ ì½˜í…ì¸ ë“¤ê³¼ ì°¨ë³„í™”í•  ìˆ˜ ìˆëŠ” ë…ì°½ì  ì ‘ê·¼ë²•**
- **ë³¸ë¬¸ ë¶„ì„ì„ í†µí•´ ë°œê²¬í•œ ì½˜í…ì¸  ê³µë°± ì˜ì—­**
- **ë” ë‚˜ì€ ì •ë³´ ì œê³µì„ ìœ„í•œ êµ¬ì²´ì ì¸ êµ¬ì„±ê³¼ ë‚´ìš© ì œì•ˆ**
- **ë…ì ë§Œì¡±ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆëŠ” ì‹¤ìš©ì  íŒê³¼ ê°€ì´ë“œë¼ì¸**

ê° ë¶„ì„ í•­ëª©ì— ëŒ€í•´ ì‹¤ì œ ì œëª©ê³¼ ë³¸ë¬¸ ë‚´ìš©ì„ ì¸ìš©í•˜ë©° êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”. 
íŠ¹íˆ ë³¸ë¬¸ ë‚´ìš© ë¶„ì„ì„ í†µí•´ ì–»ì€ ê¹Šì´ ìˆëŠ” í†µì°°ì„ ê°•ì¡°í•´ì£¼ì„¸ìš”."""

    def generate_titles_with_gpt(self, analysis_result, query, num_titles=10):
        """ìƒˆë¡œìš´ ë¸”ë¡œê·¸ ì œëª© ìƒì„± - ìˆ«ìì™€ ì œëª©ë§Œ ì¶œë ¥"""
        if not OPENAI_AVAILABLE:
            raise Exception("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if not self.openai_api_key:
            raise Exception("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        client = OpenAI(api_key=self.openai_api_key)

        # í˜„ì¬ ì‹œì  ì •ë³´ ì¶”ê°€
        from datetime import datetime
        current_year = datetime.now().year
        next_year = current_year + 1

        # ê°„ë‹¨í•˜ê³  ëª…í™•í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        system_prompt = f"""ë‹¹ì‹ ì€ ë§¤ë ¥ì ì¸ ë¸”ë¡œê·¸ ì œëª©ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
í˜„ì¬ëŠ” {current_year}ë…„ì´ë©°, ì‹œì˜ì„± ìˆëŠ” ìµœì‹  ì •ë³´ë¥¼ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ ìˆ«ìì™€ ì œëª©ë§Œ ì¶œë ¥í•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¶„ë¥˜ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

        user_prompt = f"""í‚¤ì›Œë“œ '{query}'ì— ëŒ€í•´ {num_titles}ê°œì˜ ë§¤ë ¥ì ì¸ ë¸”ë¡œê·¸ ì œëª©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ğŸ• **í˜„ì¬ ì‹œì  ì •ë³´ (ë§¤ìš° ì¤‘ìš”!)**
- í˜„ì¬ëŠ” {current_year}ë…„ì…ë‹ˆë‹¤
- ì „ë§ì´ë‚˜ ì˜ˆì¸¡ì„ ì–¸ê¸‰í•  ë•ŒëŠ” {current_year}ë…„ í•˜ë°˜ê¸°, {next_year}ë…„, {next_year + 1}ë…„ ë“± ë¯¸ë˜ ì‹œì ì„ ì‚¬ìš©í•˜ì„¸ìš”
- 2023ë…„, 2024ë…„ ê°™ì€ ê³¼ê±° ì—°ë„ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

ğŸš¨ **ì ˆëŒ€ ì¤‘ìš”í•œ ê·œì¹™** ğŸš¨
- ë°˜ë“œì‹œ "ìˆ«ì. ì œëª©" í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥
- "ìœ í˜•:", "íƒ€ê²Ÿ:", "í‚¤ì›Œë“œ:", "íŠ¹ì§•:", "ì„¤ëª…:" ë“± ì–´ë–¤ ì¶”ê°€ ì •ë³´ë„ ì ˆëŒ€ ê¸ˆì§€
- ì œëª© ë’¤ì— ì„¤ëª…ì´ë‚˜ ë¶„ë¥˜ë¥¼ ë¶™ì´ì§€ ë§ˆì„¸ìš”
- ì˜¤ì§ ì œëª©ë§Œ ì¶œë ¥í•˜ì„¸ìš”

âœ… **ì˜¬ë°”ë¥¸ ì˜ˆì‹œ:**
1. {query} ì´ˆë³´ìë¥¼ ìœ„í•œ ì™„ë²½ ê°€ì´ë“œ
2. {query} ì‹¤ì „ í™œìš©ë²• ì´ì •ë¦¬
3. {query}ë¡œ ì„±ê³µí•˜ëŠ” 3ê°€ì§€ ë°©ë²•
4. {current_year}ë…„ ìµœì‹  {query} íŠ¸ë Œë“œ ë¶„ì„
5. {next_year}ë…„ {query} ì „ë§ê³¼ ëŒ€ë¹„ì±…

âŒ **ì ˆëŒ€ ê¸ˆì§€ ì˜ˆì‹œ:**
1. {query} ì´ˆë³´ìë¥¼ ìœ„í•œ ì™„ë²½ ê°€ì´ë“œ
**ìœ í˜•:** ì •ë³´ ì œê³µí˜•
2. 2024ë…„ {query} ì „ë§ (ê³¼ê±° ì—°ë„ ì‚¬ìš© ê¸ˆì§€!)

ì§€ê¸ˆ ë°”ë¡œ {num_titles}ê°œì˜ ì œëª©ì„ ìˆ«ìì™€ ì œëª©ë§Œìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”."""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=1500,
            temperature=0.8
        )

        return response.choices[0].message.content

    def generate_blog_content(self, title, keyword, prompt_type, additional_prompt="", min_chars=6000, max_chars=12000, analysis_result=None):
        """SEO ìµœì í™”ëœ ë¸”ë¡œê·¸ ê¸€ ìƒì„±"""
        # min_charsì™€ max_charsë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ (ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¬ ê²½ìš° ëŒ€ë¹„)
        try:
            min_chars = int(min_chars) if min_chars else 6000
            max_chars = int(max_chars) if max_chars else 12000
        except (ValueError, TypeError):
            min_chars = 6000
            max_chars = 12000
            
        # SEO ìµœì í™”ë¥¼ ìœ„í•œ ìµœì†Œ ê¸€ììˆ˜ ì„¤ì • (ë¬¸ë‹¨ë³„ 1500ì Ã— 5ë¬¸ë‹¨)
        if min_chars < 7500:  # 5ë¬¸ë‹¨ Ã— 1500ì = 7500ì ìµœì†Œ
            min_chars = 7500
            print(f"ğŸ” ë¬¸ë‹¨ë³„ 1500ì êµ¬ì¡°ë¥¼ ìœ„í•´ ìµœì†Œ ê¸€ììˆ˜ë¥¼ 7500ìë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
        
        # í‚¤ì›Œë“œ ë°€ë„ ë¶„ì„ì„ ìœ„í•œ ì˜ˆìƒ í‚¤ì›Œë“œ ê°œìˆ˜ ê³„ì‚°
        target_keyword_count = max(8, min_chars // 300)  # 300ìë‹¹ 1ê°œ í‚¤ì›Œë“œ ëª©í‘œ
        
        print(f"ğŸ” SEO ìµœì í™” ëª©í‘œ:")
        print(f"  - ë©”ì¸ í‚¤ì›Œë“œ '{keyword}': {target_keyword_count}íšŒ ì´ìƒ")
        print(f"  - í‚¤ì›Œë“œ ë°€ë„: 2-3% ëª©í‘œ")
        print(f"  - LSI í‚¤ì›Œë“œ: 15-20ê°œ í¬í•¨")
        print(f"  - êµ¬ì¡°: ë„ì…ë¶€(1500ì) + ë³¸ë¬¸3ë‹¨ë½(ê°1500ì) + ê²°ë¡ ë¶€(1500ì) = ìµœì†Œ 7500ì")

        print(f"ğŸ¯ ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì‹œì‘")
        print(f"ğŸ“ ì œëª©: {title}")
        print(f"ğŸ”‘ í‚¤ì›Œë“œ: {keyword}")
        print(f"ğŸ“Š í”„ë¡¬í”„íŠ¸ ìœ í˜•: {prompt_type}")
        print(f"ğŸ“ ê¸€ììˆ˜ ë²”ìœ„: {min_chars}-{max_chars}ì")
        print(f"â• ì¶”ê°€ í”„ë¡¬í”„íŠ¸: {additional_prompt[:100] if additional_prompt else 'ì—†ìŒ'}")
        print(f"ğŸ§  ë¶„ì„ ê²°ê³¼ í¬í•¨: {'ì˜ˆ' if analysis_result else 'ì•„ë‹ˆì˜¤'}")

        if not OPENAI_AVAILABLE:
            raise Exception("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if not self.openai_api_key:
            raise Exception("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        client = OpenAI(api_key=self.openai_api_key)

        try:
            # ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ìƒì„±
            if PROMPTS_AVAILABLE:
                print("âœ… prompts.py ëª¨ë“ˆì´ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                try:
                    # ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì… í™•ì¸
                    available_prompts = prompts.get_blog_content_prompts()
                    print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ íƒ€ì…: {list(available_prompts.keys())}")

                    if prompt_type not in available_prompts:
                        print(f"âš ï¸ ìš”ì²­ëœ í”„ë¡¬í”„íŠ¸ íƒ€ì… '{prompt_type}'ì´ ì—†ìŠµë‹ˆë‹¤. 'informative'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
                        prompt_type = 'informative'

                    print(f"ğŸ¯ ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ íƒ€ì…: {prompt_type}")

                    prompt_data = prompts.create_blog_content_prompt(
                        title=title,
                        keyword=keyword,
                        prompt_type=prompt_type,
                        additional_prompt=additional_prompt,
                        min_chars=min_chars,
                        max_chars=max_chars,
                        analysis_result=analysis_result
                    )

                    system_prompt = prompt_data['system_prompt']
                    user_prompt = prompt_data['user_prompt']

                    print("âœ… prompts.pyì—ì„œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
                    print(f"ğŸ“ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(system_prompt)}ì")
                    print(f"ğŸ“ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(user_prompt)}ì")

                except Exception as e:
                    print(f"âŒ prompts.py ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    # Fallback í”„ë¡¬í”„íŠ¸
                    system_prompt, user_prompt = self._create_fallback_prompts(title, keyword, min_chars, max_chars, additional_prompt)

            else:
                print("âŒ prompts.py ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # Fallback í”„ë¡¬í”„íŠ¸
                system_prompt, user_prompt = self._create_fallback_prompts(title, keyword, min_chars, max_chars, additional_prompt)

            print("ğŸš€ OpenAI API í˜¸ì¶œ ì‹œì‘ (ì´ˆê¸° ìƒì„±)")

            # êµ¬ì¡°í™”ëœ ê¸´ ë¸”ë¡œê·¸ ê¸€ í•œ ë²ˆì— ìƒì„± (ì´ì–´ì“°ê¸° ì—†ìŒ)
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=15000,  # í† í° ìˆ˜ ì¦ê°€ë¡œ ê¸´ ê¸€ ìƒì„± ì§€ì›
                temperature=0.7
            )

            generated_content = response.choices[0].message.content
            char_count = len(generated_content)

            print(f"âœ… êµ¬ì¡°í™”ëœ ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì™„ë£Œ")
            print(f"ğŸ“ ìƒì„±ëœ ê¸€ììˆ˜: {char_count}ì")
            print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {'âœ…' if char_count >= min_chars else 'âŒ'}")
            
            if char_count >= min_chars:
                print(f"ğŸ‰ ì„±ê³µ: {char_count}ìë¡œ ëª©í‘œ {min_chars}ìë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
                print(f"ğŸ“ êµ¬ì¡°: ë„ì…ë¶€ + ë³¸ë¬¸3ë‹¨ë½ + ê²°ë¡ ë¶€ = ì´ 5ë¬¸ë‹¨ êµ¬ì„±")
            else:
                print(f"âš ï¸ ëª©í‘œ ê¸€ììˆ˜ì— ë¯¸ë‹¬í•˜ì˜€ì§€ë§Œ êµ¬ì¡°í™”ëœ ê¸€ì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {char_count}ì")
                print(f"ğŸ’¡ ë‹¤ìŒ ë²ˆì—ëŠ” ë” ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")

            # SEO ë¶„ì„ ìˆ˜í–‰
            seo_analysis = self._analyze_seo_content(generated_content, keyword)
            
            print(f"ğŸ“Š SEO ë¶„ì„ ê²°ê³¼:")
            print(f"  - í‚¤ì›Œë“œ '{keyword}' ì¶œí˜„ ë¹ˆë„: {seo_analysis['keyword_count']}íšŒ")
            print(f"  - í‚¤ì›Œë“œ ë°€ë„: {seo_analysis['keyword_density']:.1f}%")
            print(f"  - SEO ì ìˆ˜: {seo_analysis['seo_score']}/100ì ")
            
            return generated_content

        except Exception as e:
            print(f"âŒ ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise e

    

    def _analyze_seo_content(self, content, keyword):
        """ìƒì„±ëœ ì½˜í…ì¸ ì˜ SEO ìš”ì†Œ ë¶„ì„"""
        import re
        
        # í‚¤ì›Œë“œ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        keyword_count = len(re.findall(re.escape(keyword), content, re.IGNORECASE))
        
        # ì „ì²´ ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        word_count = len(content.split())
        
        # í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚° (%)
        keyword_density = (keyword_count / word_count) * 100 if word_count > 0 else 0
        
        # SEO ì ìˆ˜ ê³„ì‚° (ê¸°ë³¸ ê¸°ì¤€)
        seo_score = 0
        
        # í‚¤ì›Œë“œ ì¶œí˜„ ë¹ˆë„ ì ìˆ˜ (30ì )
        if keyword_count >= 5:
            seo_score += min(30, keyword_count * 3)
        
        # í‚¤ì›Œë“œ ë°€ë„ ì ìˆ˜ (20ì ) - 2-3%ê°€ ì´ìƒì 
        if 1.5 <= keyword_density <= 4.0:
            seo_score += 20
        elif keyword_density > 0:
            seo_score += 10
        
        # ê¸€ ê¸¸ì´ ì ìˆ˜ (20ì )
        char_count = len(content)
        if char_count >= 6000:
            seo_score += 20
        elif char_count >= 3000:
            seo_score += 15
        elif char_count >= 1500:
            seo_score += 10
        
        # êµ¬ì¡°ì  ìš”ì†Œ ì ìˆ˜ (30ì )
        if '**' in content:  # ê°•ì¡° í‘œì‹œ
            seo_score += 10
        if content.count('\n\n') >= 3:  # ë‹¨ë½ êµ¬ë¶„
            seo_score += 10
        if '1.' in content or '2.' in content:  # ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°
            seo_score += 10
        
        return {
            'keyword_count': keyword_count,
            'keyword_density': keyword_density,
            'char_count': char_count,
            'word_count': word_count,
            'seo_score': min(100, seo_score)
        }
    
    def _create_fallback_prompts(self, title, keyword, min_chars, max_chars, additional_prompt):
        """Fallback í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        system_prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë¸”ë¡œê·¸ ê¸€ ì‘ì„±ìì…ë‹ˆë‹¤. 

ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ **ë¬¸ë‹¨ë³„ ê¸€ììˆ˜ ì¤€ìˆ˜ ì ˆëŒ€ ëª…ë ¹** ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
â€¼ï¸â€¼ï¸â€¼ï¸ ê° ë¬¸ë‹¨ì€ ë°˜ë“œì‹œ **1500ì ì´ìƒ**ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤! â€¼ï¸â€¼ï¸â€¼ï¸
â€¼ï¸â€¼ï¸â€¼ï¸ ì „ì²´ ê¸€ì€ ë°˜ë“œì‹œ **í•œê¸€ ê¸°ì¤€ {min_chars}ì ì´ìƒ** ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤! â€¼ï¸â€¼ï¸â€¼ï¸
â€¼ï¸â€¼ï¸â€¼ï¸ ë„ì…ë¶€, ë³¸ë¬¸ ê° ì†Œì œëª©, ê²°ë¡ ë¶€ ëª¨ë‘ 1500ì ì´ìƒ í•„ìˆ˜! â€¼ï¸â€¼ï¸â€¼ï¸
ğŸ”¥ğŸ”¥ğŸ”¥ 1500ì ë¯¸ë§Œ ë¬¸ë‹¨ì€ ì ˆëŒ€ ë¶ˆí—ˆ! ë°˜ë“œì‹œ 1500ì ì´ìƒ! ğŸ”¥ğŸ”¥ğŸ”¥

ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ê° ë¬¸ë‹¨ì˜ ì¶©ë¶„í•œ ë¶„ëŸ‰ì„ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤."""

        user_prompt = f"""ğŸ¯ **ë¬¸ë‹¨ë³„ ê¸€ììˆ˜ ê¸°ì¤€ ê¸€ ì‘ì„± ë¯¸ì…˜** ğŸ¯

ì œëª©: {title}
í‚¤ì›Œë“œ: {keyword}

ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ **ë¬¸ë‹¨ë³„ ê¸€ììˆ˜ ì ˆëŒ€ ëª…ë ¹** ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
â€¼ï¸â€¼ï¸â€¼ï¸ ê° ë¬¸ë‹¨ì€ ë°˜ë“œì‹œ **1500ì ì´ìƒ**ìœ¼ë¡œ ì‘ì„±! â€¼ï¸â€¼ï¸â€¼ï¸
â€¼ï¸â€¼ï¸â€¼ï¸ ì „ì²´ ê¸€ì€ ë°˜ë“œì‹œ **í•œê¸€ ê¸°ì¤€ {min_chars}ì ì´ìƒ** ì‘ì„±! â€¼ï¸â€¼ï¸â€¼ï¸
â€¼ï¸â€¼ï¸â€¼ï¸ ìµœëŒ€ {max_chars}ìê¹Œì§€ ì‘ì„± ê°€ëŠ¥! â€¼ï¸â€¼ï¸â€¼ï¸
ğŸ”¥ğŸ”¥ğŸ”¥ 1500ì ë¯¸ë§Œ ë¬¸ë‹¨ì€ ì ˆëŒ€ ë¶ˆí—ˆ! ë°˜ë“œì‹œ 1500ì ì´ìƒ! ğŸ”¥ğŸ”¥ğŸ”¥

ğŸ’ªğŸ’ªğŸ’ª **ë¬¸ë‹¨ë³„ ê¸€ììˆ˜ ì ˆëŒ€ ê¸°ì¤€** ğŸ’ªğŸ’ªğŸ’ª

1. **ë„ì…ë¶€** (1500ì ì´ìƒ ì ˆëŒ€ í•„ìˆ˜!)
   - ë…ìì˜ ê´€ì‹¬ì„ ë„ëŠ” í¥ë¯¸ë¡œìš´ ì‹œì‘
   - ì£¼ì œ ì†Œê°œì™€ ê¸€ì˜ ëª©ì 
   - ë…ìê°€ ì–»ì„ ìˆ˜ ìˆëŠ” ê°€ì¹˜ ì œì‹œ
   - ê°œì¸ì ì¸ ê²½í—˜ì´ë‚˜ ê³µê° ìš”ì†Œ í¬í•¨
   - êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ ë°°ê²½ ì„¤ëª… ì¶”ê°€

2. **ë³¸ë¬¸ 1ë‹¨ë½** (1500ì ì´ìƒ ì ˆëŒ€ í•„ìˆ˜!)
   - í•µì‹¬ ë‚´ìš©ì˜ ì²« ë²ˆì§¸ ìš”ì†Œ
   - êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ì˜ˆì‹œ
   - ì‹¤ì œ ì‚¬ë¡€ë‚˜ ê²½í—˜ë‹´ í¬í•¨
   - ë‹¨ê³„ë³„ ë°©ë²•ë¡ ê³¼ ì‹¤ìš©ì  íŒ
   - ë…ìì˜ ìƒí™©ë³„ ì ìš© ë°©ë²•

3. **ë³¸ë¬¸ 2ë‹¨ë½** (1500ì ì´ìƒ ì ˆëŒ€ í•„ìˆ˜!)
   - í•µì‹¬ ë‚´ìš©ì˜ ë‘ ë²ˆì§¸ ìš”ì†Œ
   - ì‹¬í™”ëœ ë‚´ìš©ê³¼ ì¶”ê°€ ì¸ì‚¬ì´íŠ¸
   - ë‹¤ì–‘í•œ ê´€ì ê³¼ ì ‘ê·¼ë²• ì œì‹œ
   - ì£¼ì˜ì‚¬í•­ê³¼ ë¬¸ì œ í•´ê²° ë°©ë²•
   - ì„±ê³µ ì‚¬ë¡€ì™€ ì‹¤íŒ¨ ì‚¬ë¡€ ë¹„êµ

4. **ê²°ë¡ ë¶€** (1500ì ì´ìƒ ì ˆëŒ€ í•„ìˆ˜!)
   - ë‚´ìš© ìš”ì•½ê³¼ í•µì‹¬ ë©”ì‹œì§€
   - ë…ìë¥¼ ìœ„í•œ ë§ˆì§€ë§‰ ì¡°ì–¸
   - í–‰ë™ ìœ ë„ì™€ ê²©ë ¤ ë©”ì‹œì§€
   - ì‹¤ì²œ ë°©ë²•ê³¼ ë‹¨ê³„ë³„ ê°€ì´ë“œ
   - í•´ì‹œíƒœê·¸ 15-20ê°œ í¬í•¨

ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ **ê·¹ë„ë¡œ ì¤‘ìš”í•œ ì‘ì„± ê·œì¹™** ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
â€¼ï¸â€¼ï¸â€¼ï¸ ê° ë¬¸ë‹¨ì„ 1500ì ì´ìƒìœ¼ë¡œ ê·¹ë„ë¡œ ìì„¸í•˜ê²Œ! â€¼ï¸â€¼ï¸â€¼ï¸
â€¼ï¸â€¼ï¸â€¼ï¸ ì˜ˆì‹œì™€ ê²½í—˜ë‹´ì„ ëŒ€ëŸ‰ í¬í•¨! â€¼ï¸â€¼ï¸â€¼ï¸
â€¼ï¸â€¼ï¸â€¼ï¸ ê°ì •ê³¼ ìƒê°ì„ í’ë¶€í•˜ê²Œ í‘œí˜„! â€¼ï¸â€¼ï¸â€¼ï¸
â€¼ï¸â€¼ï¸â€¼ï¸ {min_chars}ì ë¯¸ë§Œì´ë©´ ë” ë§ì€ ë‚´ìš© ì¶”ê°€! â€¼ï¸â€¼ï¸â€¼ï¸
â€¼ï¸â€¼ï¸â€¼ï¸ ê¸€ì„ ì¤‘ê°„ì— ëŠì§€ ë§ê³  ëê¹Œì§€! â€¼ï¸â€¼ï¸â€¼ï¸

## ê¸€ ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ í¬í•¨:
**í•´ì‹œíƒœê·¸ ì¶”ì²œ:** (15-20ê°œ)
ê´€ë ¨ í•´ì‹œíƒœê·¸ë“¤ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”."""

        if additional_prompt:
            user_prompt += f"\n\nì¶”ê°€ ìš”ì²­ì‚¬í•­:\n{additional_prompt}"

        return system_prompt, user_prompt

    def _create_fallback_title_prompt(self, query, analysis_result, num_titles):
        """Fallback ì œëª© ìƒì„± í”„ë¡¬í”„íŠ¸"""
        return f"""í‚¤ì›Œë“œ '{query}'ì— ëŒ€í•´ {num_titles}ê°œì˜ ë§¤ë ¥ì ì¸ ë¸”ë¡œê·¸ ì œëª©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ğŸ¯ **ì œëª© ìƒì„± ìš”êµ¬ì‚¬í•­**
- í´ë¦­í•˜ê³  ì‹¶ê²Œ ë§Œë“œëŠ” ë§¤ë ¥ì ì¸ ì œëª©
- SEOì— ìµœì í™”ëœ í‚¤ì›Œë“œ í¬í•¨
- ë…ìì˜ í˜¸ê¸°ì‹¬ì„ ìê·¹í•˜ëŠ” ë‚´ìš©
- ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ê³¼ ì ‘ê·¼ë²• ì‚¬ìš©

ğŸ“‹ **ì¶œë ¥ í˜•ì‹**
ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”:

1. [ì²« ë²ˆì§¸ ì œëª©]
2. [ë‘ ë²ˆì§¸ ì œëª©]
3. [ì„¸ ë²ˆì§¸ ì œëª©]
...
{num_titles}. [ë§ˆì§€ë§‰ ì œëª©]

âš ï¸ **ì¤‘ìš”í•œ ê·œì¹™**
- ê° ì œëª© ì•ì—ëŠ” ë°˜ë“œì‹œ "ìˆ«ì. " í˜•ì‹ìœ¼ë¡œ ë²ˆí˜¸ í‘œì‹œ
- ì œëª© ì™¸ì— ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¶„ë¥˜ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ
- "ìœ í˜•:", "íƒ€ê²Ÿ:", "ëª©ì :" ë“±ì˜ ì¶”ê°€ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ ê²ƒ
- ì˜¤ì§ ì œëª©ë§Œ ì¶œë ¥í•  ê²ƒ

{f'ğŸ“Š ë¶„ì„ ê²°ê³¼ ì°¸ê³ :{chr(10)}{analysis_result[:500]}...' if analysis_result else ''}

ì§€ê¸ˆ ë°”ë¡œ {num_titles}ê°œì˜ ì œëª©ì„ ìœ„ í˜•ì‹ìœ¼ë¡œë§Œ ìƒì„±í•´ì£¼ì„¸ìš”."""

    def generate_dall_e_images(self, title, content, keyword, num_images=4):
        """DALL-E ì´ë¯¸ì§€ ìƒì„±"""
        if not OPENAI_AVAILABLE:
            raise Exception("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if not self.openai_api_key:
            raise Exception("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        client = OpenAI(api_key=self.openai_api_key)

        # ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompts_text = self.create_image_prompts(title, content, keyword, num_images)

        generated_images = []

        for i, prompt in enumerate(prompts_text[:num_images]):
            try:
                response = client.images.generate(
                    model="dall-e-3",
                    prompt=prompt,
                    size="1024x1024",
                    quality="standard",
                    n=1
                )

                if response and response.data and len(response.data) > 0:
                    image_url = response.data[0].url
                    generated_images.append({
                        'prompt': prompt,
                        'url': image_url,
                        'index': i+1
                    })

                time.sleep(1)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ

            except Exception as e:
                print(f"ì´ë¯¸ì§€ {i+1} ìƒì„± ì˜¤ë¥˜: {str(e)}")
                continue

        return generated_images

    def create_image_prompts(self, title, content, keyword, num_images=4):
        """ë¸”ë¡œê·¸ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        try:
            # GPTë¡œ ì½˜í…ì¸  ë¶„ë¥˜ ë° ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
            client = OpenAI(api_key=self.openai_api_key)

            prompt_request = f"""
ë‹¤ìŒ ë¸”ë¡œê·¸ ê¸€ì„ ë¶„ì„í•˜ì—¬ ì´ë¯¸ì§€ ìœ í˜•ì„ ë¶„ë¥˜í•˜ê³ , ì „ë¬¸ì ì¸ ì‚¬ì§„ ìŠ¤íƒ€ì¼ì˜ DALL-E í”„ë¡¬í”„íŠ¸ {num_images}ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
í‚¤ì›Œë“œ: {keyword}

ë¸”ë¡œê·¸ ë‚´ìš©:
{content[:2000]}

=== ì‘ì—… ìˆœì„œ ===
1ë‹¨ê³„: ê¸€ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì£¼ìš” ì´ë¯¸ì§€ ìœ í˜•ì„ "ì¸ë¬¼", "ìì—°", "ì œí’ˆ/ì‚¬ë¬¼" ì¤‘ì—ì„œ ë¶„ë¥˜
2ë‹¨ê³„: ë¶„ë¥˜ëœ ìœ í˜•ì— ë§ëŠ” ì „ë¬¸ ì‚¬ì§„ í…œí”Œë¦¿ì„ ì ìš©í•˜ì—¬ 4ê°œì˜ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±

=== ìœ í˜•ë³„ í…œí”Œë¦¿ ===
[ì¸ë¬¼ ìœ í˜•]: "Professional photography portrait of {{ì¸ë¬¼ ë¬˜ì‚¬}}, natural lighting, soft background blur, Canon EOS R5, 85mm lens, realistic skin texture, high resolution, 4K quality"

[ìì—° ìœ í˜•]: "High-resolution landscape photography of {{ìì—°í™˜ê²½ ë¬˜ì‚¬}}, vivid color grading, cinematic lighting, Sony Alpha 7R IV, 24mm lens, realistic environmental textures, ultra-detailed, 4K quality"

[ì œí’ˆ/ì‚¬ë¬¼ ìœ í˜•]: "Professional product photography of {{ì œí’ˆ ë˜ëŠ” ì‚¬ë¬¼ ë¬˜ì‚¬}}, minimalist background, subtle studio lighting, Nikon D850, macro lens, sharp details, realistic textures, ultra-detailed, 4K quality"

=== í”„ë¡¬í”„íŠ¸ ìƒì„± ê·œì¹™ ===
- ê° í”„ë¡¬í”„íŠ¸ëŠ” ë¸”ë¡œê·¸ ë‚´ìš©ì˜ ë‹¤ë¥¸ ì¸¡ë©´ì´ë‚˜ ì¥ë©´ì„ ë°˜ì˜
- ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ë™ì¼í•œ í…œí”Œë¦¿ êµ¬ì¡° ì‚¬ìš©, ë‚´ìš©ë§Œ ì°¨ë³„í™”
- í•œêµ­ì  ëŠë‚Œì´ë‚˜ ì•„ì‹œì•„ì¸ íŠ¹ì§• ë°˜ì˜ (ë³„ë„ ì–¸ê¸‰ ì—†ìœ¼ë©´)
- ì˜ì–´ë¡œ ì‘ì„±, êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë¬˜ì‚¬ í¬í•¨

ì¶œë ¥ í˜•ì‹:
ë¶„ë¥˜ëœ ìœ í˜•: [ì¸ë¬¼/ìì—°/ì œí’ˆì‚¬ë¬¼]
1. [ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸]
2. [ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸]
3. [ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸]
4. [ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸]
"""

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ì‚¬ì§„ì‘ê°€ì´ì DALL-E í”„ë¡¬í”„íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¸”ë¡œê·¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ìœ í˜•ì„ ë¶„ë¥˜í•˜ê³ , ê³ í’ˆì§ˆì˜ ì „ë¬¸ì ì¸ ì‚¬ì§„ ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt_request}
                ],
                max_tokens=1500,
                temperature=0.7
            )

            prompts_text = response.choices[0].message.content
            print(f"Generated prompts text: {prompts_text}")

            # í”„ë¡¬í”„íŠ¸ íŒŒì‹±
            prompts = []
            if prompts_text and prompts_text.strip():
                lines = prompts_text.split('\n')
                for line in lines:
                    line = line.strip()
                    # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ë¼ì¸ ì°¾ê¸°
                    if re.match(r'^\d+\.\s*', line):
                        prompt = re.sub(r'^\d+\.\s*', '', line).strip()
                        if prompt and len(prompt) > 20:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                            prompts.append(prompt)

            # ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ ë°±ì—… í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            if len(prompts) < num_images:
                backup_prompts = self.get_backup_professional_prompts(title, keyword, num_images)
                prompts.extend(backup_prompts[len(prompts):])

            return prompts[:num_images]

        except Exception as e:
            print(f"ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return self.get_backup_professional_prompts(title, keyword, num_images)

    def get_backup_professional_prompts(self, title, keyword, num_images=4):
        """ë°±ì—…ìš© ì „ë¬¸ ì‚¬ì§„ ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ìœ í˜• ì¶”ì¸¡
        person_keywords = ['ì‚¬ëŒ', 'ì¸ë¬¼', 'ì§ì—…', 'ìƒí™œ', 'ì¼ìƒ', 'ê´€ê³„', 'ê°€ì¡±', 'ì¹œêµ¬', 'ì—°ì¸', 'ì•„ì´', 'ì–´ë¥¸', 'ë‚¨ì„±', 'ì—¬ì„±']
        nature_keywords = ['ìì—°', 'ì‚°', 'ë°”ë‹¤', 'ê°•', 'ìˆ²', 'ê½ƒ', 'ë‚˜ë¬´', 'í’ê²½', 'í•˜ëŠ˜', 'êµ¬ë¦„', 'ì¼ì¶œ', 'ì¼ëª°', 'ê³„ì ˆ', 'ë‚ ì”¨']
        product_keywords = ['ì œí’ˆ', 'ìŒì‹', 'ìš”ë¦¬', 'ê¸°ìˆ ', 'ë„êµ¬', 'ì¥ë¹„', 'ë¬¼ê±´', 'ì•„ì´í…œ', 'ë¸Œëœë“œ', 'ì„œë¹„ìŠ¤']

        content_type = "ì œí’ˆ/ì‚¬ë¬¼"  # ê¸°ë³¸ê°’
        
        title_lower = title.lower()
        keyword_lower = keyword.lower()
        
        if any(word in title_lower or word in keyword_lower for word in person_keywords):
            content_type = "ì¸ë¬¼"
        elif any(word in title_lower or word in keyword_lower for word in nature_keywords):
            content_type = "ìì—°"

        if content_type == "ì¸ë¬¼":
            return [
                f"Professional photography portrait of a smiling Korean person enjoying {keyword}, natural lighting, soft background blur, Canon EOS R5, 85mm lens, realistic skin texture, high resolution, 4K quality",
                f"Professional photography portrait of Korean people discussing {keyword}, natural lighting, soft background blur, Canon EOS R5, 85mm lens, realistic skin texture, high resolution, 4K quality",
                f"Professional photography portrait of a Korean professional working with {keyword}, natural lighting, soft background blur, Canon EOS R5, 85mm lens, realistic skin texture, high resolution, 4K quality",
                f"Professional photography portrait of Korean family members experiencing {keyword} together, natural lighting, soft background blur, Canon EOS R5, 85mm lens, realistic skin texture, high resolution, 4K quality"
            ][:num_images]
        elif content_type == "ìì—°":
            return [
                f"High-resolution landscape photography of Korean natural scenery featuring {keyword}, vivid color grading, cinematic lighting, Sony Alpha 7R IV, 24mm lens, realistic environmental textures, ultra-detailed, 4K quality",
                f"High-resolution landscape photography of beautiful Korean mountains and {keyword}, vivid color grading, cinematic lighting, Sony Alpha 7R IV, 24mm lens, realistic environmental textures, ultra-detailed, 4K quality",
                f"High-resolution landscape photography of Korean traditional garden with {keyword} elements, vivid color grading, cinematic lighting, Sony Alpha 7R IV, 24mm lens, realistic environmental textures, ultra-detailed, 4K quality",
                f"High-resolution landscape photography of Korean coastal area showcasing {keyword}, vivid color grading, cinematic lighting, Sony Alpha 7R IV, 24mm lens, realistic environmental textures, ultra-detailed, 4K quality"
            ][:num_images]
        else:  # ì œí’ˆ/ì‚¬ë¬¼
            return [
                f"Professional product photography of premium {keyword} items, minimalist background, subtle studio lighting, Nikon D850, macro lens, sharp details, realistic textures, ultra-detailed, 4K quality",
                f"Professional product photography of modern {keyword} design, minimalist background, subtle studio lighting, Nikon D850, macro lens, sharp details, realistic textures, ultra-detailed, 4K quality",
                f"Professional product photography of elegant {keyword} arrangement, minimalist background, subtle studio lighting, Nikon D850, macro lens, sharp details, realistic textures, ultra-detailed, 4K quality",
                f"Professional product photography of innovative {keyword} concept, minimalist background, subtle studio lighting, Nikon D850, macro lens, sharp details, realistic textures, ultra-detailed, 4K quality"
            ][:num_images]

    def get_realtime_popular_keywords(self):
        """ë„¤ì´ë²„ ì‹¤ì‹œê°„ ì¸ê¸°ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸° (ëŒ€ì²´ ë°©ë²•)"""
        try:
            # DataLab API ëŒ€ì‹  ë¸”ë¡œê·¸ ê²€ìƒ‰ ê¸°ë°˜ìœ¼ë¡œ ì¸ê¸°ë„ ì¸¡ì •
            return self.get_naver_realtime_search()
        except Exception as e:
            print(f"ì‹¤ì‹œê°„ ì¸ê¸°ê²€ìƒ‰ì–´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return self.get_fallback_trending_keywords()

    def process_datalab_result(self, result):
        """DataLab ê²°ê³¼ ì²˜ë¦¬"""
        trending_keywords = []

        if 'results' in result:
            for group in result['results']:
                group_name = group.get('title', '')
                data = group.get('data', [])

                if data:
                    # ìµœê·¼ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                    latest_data = sorted(data, key=lambda x: x['period'], reverse=True)
                    if latest_data:
                        trending_keywords.append({
                            'category': group_name,
                            'keywords': group.get('keywords', []),
                            'trend_score': latest_data[0]['ratio']
                        })

        return trending_keywords

    def get_fallback_trending_keywords(self):
        """ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ API ì‹¤íŒ¨ì‹œ ëŒ€ì²´ íŠ¸ë Œë”© í‚¤ì›Œë“œ"""
        return [
            {'category': 'ğŸ”¥ AI/í…Œí¬', 'keywords': ['ChatGPT', 'Claude', 'Gemini', 'AIê·¸ë¦¼', 'ì½”ë”©'], 'trend_score': 95},
            {'category': 'ğŸ’° ì¬í…Œí¬', 'keywords': ['ì£¼ì‹', 'ë¹„íŠ¸ì½”ì¸', 'ë¶€ë™ì‚°', 'ê¸ˆë¦¬', 'í™˜ìœ¨'], 'trend_score': 88},
            {'category': 'ğŸƒâ€â™€ï¸ ê±´ê°•', 'keywords': ['ë‹¤ì´ì–´íŠ¸', 'í™ˆíŠ¸', 'í•„ë¼í…ŒìŠ¤', 'ë‹¨ë°±ì§ˆ', 'ìˆ˜ë©´'], 'trend_score': 82},
            {'category': 'ğŸ³ ìš”ë¦¬', 'keywords': ['ì—ì–´í”„ë¼ì´ì–´', 'ë‹¤ì´ì–´íŠ¸ì‹ë‹¨', 'ê°„í¸ìš”ë¦¬', 'ë² ì´í‚¹', 'ë„ì‹œë½'], 'trend_score': 78},
            {'category': 'âœˆï¸ ì—¬í–‰', 'keywords': ['ì œì£¼ë„', 'ë¶€ì‚°ì—¬í–‰', 'í•´ì™¸ì—¬í–‰', 'ìº í•‘', 'í˜¸í…”'], 'trend_score': 75}
        ]

    def get_naver_blog_data(self, query, display=20):
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API í˜¸ì¶œ"""
        if not self.naver_client_id or not self.naver_client_secret:
            raise Exception("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        url = "https://openapi.naver.com/v1/search/blog.json"
        headers = {
            "X-Naver-Client-Id": self.naver_client_id,
            "X-Naver-Client-Secret": self.naver_client_secret
        }
        params = {
            "query": query,
            "display": display,
            "sort": "sim"
        }

        try:
            #Removed import requests since it wasn't being used and was causing import error.
            #import requests
            #response = requests.get(url, headers=headers, params=params)
            #response.raise_for_status()
            #return response.json()
            return None #Returning None as the request part is commented out
        except Exception as e:
            print(f"ë„¤ì´ë²„ API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
            return None

    def get_google_trending_keywords(self):
        """Google Trendsì—ì„œ ì‹¤ì‹œê°„ íŠ¸ë Œë”© í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not TRENDS_AVAILABLE:
                return self.get_fallback_google_keywords()
            
            pytrends = TrendReq(hl='ko', tz=540)  # í•œêµ­ ì‹œê°„ëŒ€
            
            # ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ê°€ì ¸ì˜¤ê¸°
            trending_searches_df = pytrends.trending_searches(pn='south_korea')
            
            if trending_searches_df is not None and not trending_searches_df.empty:
                trending_keywords = []
                for i, keyword in enumerate(trending_searches_df[0].head(10)):
                    trending_keywords.append({
                        'rank': i + 1,
                        'keyword': keyword,
                        'source': 'Google Trends',
                        'category': 'ğŸ”¥ ì‹¤ì‹œê°„ ì¸ê¸°'
                    })
                
                print(f"âœ… Google Trendsì—ì„œ {len(trending_keywords)}ê°œ í‚¤ì›Œë“œ ìˆ˜ì§‘")
                return trending_keywords
            else:
                return self.get_fallback_google_keywords()
                
        except Exception as e:
            print(f"Google Trends API ì˜¤ë¥˜: {str(e)}")
            return self.get_fallback_google_keywords()
    
    def get_naver_datalab_keywords(self):
        """ë„¤ì´ë²„ DataLabì—ì„œ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not self.client_id or not self.client_secret:
                return self.get_fallback_naver_keywords()
            
            # ë„¤ì´ë²„ DataLab API í˜¸ì¶œ
            url = "https://openapi.naver.com/v1/datalab/search"
            headers = {
                "X-Naver-Client-Id": self.client_id,
                "X-Naver-Client-Secret": self.client_secret,
                "Content-Type": "application/json"
            }
            
            # ìµœê·¼ 1ì£¼ì¼ ì¸ê¸° í‚¤ì›Œë“œ ê²€ìƒ‰
            from datetime import datetime, timedelta
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
            
            # ì¸ê¸° í‚¤ì›Œë“œë“¤ë¡œ ê²€ìƒ‰ëŸ‰ ë¹„êµ
            popular_keywords = ['ChatGPT', 'ë‹¤ì´ì–´íŠ¸', 'ë¶€ë™ì‚°', 'ì£¼ì‹', 'ì—¬í–‰', 'ìš”ë¦¬', 'ìš´ë™', 'ì˜í™”', 'ê²Œì„', 'ì‡¼í•‘']
            
            body = {
                "startDate": start_date,
                "endDate": end_date,
                "timeUnit": "date",
                "keywordGroups": [{"groupName": keyword, "keywords": [keyword]} for keyword in popular_keywords[:5]]
            }
            
            response = requests.post(url, headers=headers, json=body)
            
            if response.status_code == 200:
                data = response.json()
                naver_keywords = []
                
                for i, group in enumerate(data.get('results', [])):
                    keyword = group.get('title', '')
                    # ìµœê·¼ ê²€ìƒ‰ëŸ‰ ê³„ì‚°
                    recent_data = group.get('data', [])
                    if recent_data:
                        avg_ratio = sum(item['ratio'] for item in recent_data[-3:]) / len(recent_data[-3:])
                        naver_keywords.append({
                            'rank': i + 1,
                            'keyword': keyword,
                            'source': 'Naver DataLab',
                            'category': 'ğŸ“Š ê²€ìƒ‰ëŸ‰ ìƒìœ„',
                            'trend_score': round(avg_ratio, 1)
                        })
                
                print(f"âœ… ë„¤ì´ë²„ DataLabì—ì„œ {len(naver_keywords)}ê°œ í‚¤ì›Œë“œ ìˆ˜ì§‘")
                return naver_keywords
            else:
                print(f"ë„¤ì´ë²„ DataLab API ì˜¤ë¥˜: {response.status_code}")
                return self.get_fallback_naver_keywords()
                
        except Exception as e:
            print(f"ë„¤ì´ë²„ DataLab API ì˜¤ë¥˜: {str(e)}")
            return self.get_fallback_naver_keywords()
    
    def get_fallback_google_keywords(self):
        """Google Trends ì‹¤íŒ¨ì‹œ ëŒ€ì²´ í‚¤ì›Œë“œ"""
        return [
            {'rank': 1, 'keyword': 'ChatGPT', 'source': 'Google Trends (Fallback)', 'category': 'ğŸ¤– AI'},
            {'rank': 2, 'keyword': 'ë‹¤ì´ì–´íŠ¸', 'source': 'Google Trends (Fallback)', 'category': 'ğŸ’ª ê±´ê°•'},
            {'rank': 3, 'keyword': 'ë¶€ë™ì‚°', 'source': 'Google Trends (Fallback)', 'category': 'ğŸ  íˆ¬ì'},
            {'rank': 4, 'keyword': 'ì—¬í–‰', 'source': 'Google Trends (Fallback)', 'category': 'âœˆï¸ ì—¬í–‰'},
            {'rank': 5, 'keyword': 'ì£¼ì‹', 'source': 'Google Trends (Fallback)', 'category': 'ğŸ“ˆ ê¸ˆìœµ'}
        ]
    
    def get_fallback_naver_keywords(self):
        """ë„¤ì´ë²„ DataLab ì‹¤íŒ¨ì‹œ ëŒ€ì²´ í‚¤ì›Œë“œ"""
        return [
            {'rank': 1, 'keyword': 'ìš”ë¦¬ë ˆì‹œí”¼', 'source': 'Naver DataLab (Fallback)', 'category': 'ğŸ³ ìš”ë¦¬'},
            {'rank': 2, 'keyword': 'í™ˆíŠ¸ë ˆì´ë‹', 'source': 'Naver DataLab (Fallback)', 'category': 'ğŸ’ª ìš´ë™'},
            {'rank': 3, 'keyword': 'ë„·í”Œë¦­ìŠ¤', 'source': 'Naver DataLab (Fallback)', 'category': 'ğŸ¬ ì—”í„°'},
            {'rank': 4, 'keyword': 'ì¸í…Œë¦¬ì–´', 'source': 'Naver DataLab (Fallback)', 'category': 'ğŸ¡ í™ˆ'},
            {'rank': 5, 'keyword': 'ì‚¬ì´ë“œì¡', 'source': 'Naver DataLab (Fallback)', 'category': 'ğŸ’¼ ë¶€ì—…'}
        ]



    def get_fallback_popular_keywords(self):
        """API ì‹¤íŒ¨ì‹œ ëŒ€ì²´ ì¸ê¸° í‚¤ì›Œë“œ"""
        return [
            {'keyword': 'ChatGPT', 'result_count': 50000, 'popularity_score': 95},
            {'keyword': 'ë‹¤ì´ì–´íŠ¸', 'result_count': 45000, 'popularity_score': 88},
            {'keyword': 'ë¶€ë™ì‚°', 'result_count': 40000, 'popularity_score': 82},
            {'keyword': 'ì£¼ì‹', 'result_count': 38000, 'popularity_score': 78},
            {'keyword': 'ì—¬í–‰', 'result_count': 35000, 'popularity_score': 75},
            {'keyword': 'ìš”ë¦¬', 'result_count': 32000, 'popularity_score': 72},
            {'keyword': 'ìš´ë™', 'result_count': 30000, 'popularity_score': 68},
            {'keyword': 'ì˜í™”', 'result_count': 28000, 'popularity_score': 65},
            {'keyword': 'ê²Œì„', 'result_count': 25000, 'popularity_score': 62},
            {'keyword': 'íŒ¨ì…˜', 'result_count': 22000, 'popularity_score': 58}
        ]

    def get_predefined_categories(self):
        """ë¯¸ë¦¬ ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ë“¤ ë°˜í™˜"""
        return {
            "ğŸ”¥ íŠ¸ë Œë“œ/í•«ì´ìŠˆ": {
                "keywords": ["ChatGPT", "ì¸ê³µì§€ëŠ¥", "ë©”íƒ€ë²„ìŠ¤", "NFT", "ê°€ìƒí™”í", "ì „ê¸°ì°¨", "ESG", "êµ¬ë…ê²½ì œ", "MZì„¸ëŒ€", "í´ë¦°ë·°í‹°", "ë¹„ê±´", "ì œë¡œì›¨ì´ìŠ¤íŠ¸", "í‹±í†¡", "ë¦´ìŠ¤", "ìˆí¼", "AIê·¸ë¦¼"],
                "description": "ìµœì‹  í™”ì œ, ë‰´ìŠ¤, ì‚¬íšŒì  ì´ìŠˆ",
                "color": "linear-gradient(135deg, #ff6b6b, #ee5a24)"
            },
            "ğŸ’° ì¬í…Œí¬/íˆ¬ì": {
                "keywords": ["ì£¼ì‹", "ë¶€ë™ì‚°", "ê°€ìƒí™”í", "ë¹„íŠ¸ì½”ì¸", "íˆ¬ì", "í€ë“œ", "ì ê¸ˆ", "ì—°ê¸ˆ", "ì„¸ê¸ˆ", "ì ˆì„¸", "íŒŒì´ì–´ì¡±", "ê²½ì œë…ë¦½", "ì½”ì¸", "ETF", "ì±„ê¶Œ", "ë°°ë‹¹ì£¼"],
                "description": "ì£¼ì‹, ë¶€ë™ì‚°, íˆ¬ì ê´€ë ¨ ì •ë³´",
                "color": "linear-gradient(135deg, #2ecc71, #27ae60)"
            },
            "ğŸƒâ€â™€ï¸ ê±´ê°•/í”¼íŠ¸ë‹ˆìŠ¤": {
                "keywords": ["ë‹¤ì´ì–´íŠ¸", "í™ˆíŠ¸ë ˆì´ë‹", "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "ëŸ¬ë‹", "í—¬ìŠ¤", "ë‹¨ë°±ì§ˆ", "ë¹„íƒ€ë¯¼", "ìˆ˜ë©´", "ìŠ¤íŠ¸ë ˆì¹­", "ë§ˆë¼í†¤", "í¬ë¡œìŠ¤í•", "PT", "ì²´ì¤‘ê°ëŸ‰", "ê·¼ë ¥ìš´ë™", "ìœ ì‚°ì†Œ"],
                "description": "ìš´ë™, ë‹¤ì´ì–´íŠ¸, ê±´ê°•ê´€ë¦¬",
                "color": "linear-gradient(135deg, #3498db, #2980b9)"
            },
            "ğŸ³ ìš”ë¦¬/ë ˆì‹œí”¼": {
                "keywords": ["ì—ì–´í”„ë¼ì´ì–´", "í™ˆì¿¡", "ë‹¤ì´ì–´íŠ¸ì‹ë‹¨", "ê°„í¸ìš”ë¦¬", "ë² ì´í‚¹", "ë¹„ê±´ìš”ë¦¬", "í‚¤í† ì‹ë‹¨", "ë„ì‹œë½", "ë¸ŒëŸ°ì¹˜", "ë””ì €íŠ¸", "ë°œíš¨ìŒì‹", "í•œì‹", "ì–‘ì‹", "ì¤‘ì‹", "ì¼ì‹", "ë¶„ì‹"],
                "description": "ìŒì‹, ìš”ë¦¬ë²•, ë§›ì§‘ ì •ë³´",
                "color": "linear-gradient(135deg, #f39c12, #e67e22)"
            },
            "âœˆï¸ ì—¬í–‰/ê´€ê´‘": {
                "keywords": ["ì œì£¼ë„", "ë¶€ì‚°", "ê°•ë¦‰", "ê²½ì£¼", "ì „ì£¼", "í•´ì™¸ì—¬í–‰", "ìº í•‘", "ê¸€ë¨í•‘", "í˜¸í…”", "íœì…˜", "ë°°ë‚­ì—¬í–‰", "íŒ¨í‚¤ì§€ì—¬í–‰", "ì¼ë³¸ì—¬í–‰", "ìœ ëŸ½ì—¬í–‰", "ë™ë‚¨ì•„ì—¬í–‰", "êµ­ë‚´ì—¬í–‰"],
                "description": "êµ­ë‚´ì™¸ ì—¬í–‰, ëª…ì†Œ, ìˆ™ë°•",
                "color": "linear-gradient(135deg, #9b59b6, #8e44ad)"
            },
            "ğŸ® ì·¨ë¯¸/ì—”í„°": {
                "keywords": ["ë…ì„œ", "ì˜í™”", "ê²Œì„", "ë“œë¼ë§ˆ", "ì›¹íˆ°", "ìŒì•…", "ê·¸ë¦¼", "ì‚¬ì§„", "ë„·í”Œë¦­ìŠ¤", "ìœ íŠœë¸Œ", "ìŠ¤íŠ¸ë¦¬ë°", "OTT", "ì• ë‹ˆë©”ì´ì…˜", "KíŒ", "ë“œë¼ë§ˆì¶”ì²œ", "ì˜í™”ì¶”ì²œ"],
                "description": "ê²Œì„, ì˜í™”, ë“œë¼ë§ˆ, ìŒì•…",
                "color": "linear-gradient(135deg, #e74c3c, #c0392b)"
            },
            "ğŸ‘” ë¹„ì¦ˆë‹ˆìŠ¤/ë§ˆì¼€íŒ…": {
                "keywords": ["ì°½ì—…", "ë¶€ì—…", "ë§ˆì¼€íŒ…", "ë¸Œëœë”©", "SNSë§ˆì¼€íŒ…", "ìœ íŠœë¸Œ", "ë¸”ë¡œê·¸", "ì˜¨ë¼ì¸ì‡¼í•‘", "ì´ì»¤ë¨¸ìŠ¤", "ìŠ¤íƒ€íŠ¸ì—…", "í”„ë¦¬ëœì„œ", "ì‚¬ì´ë“œì¡", "ë””ì§€í„¸ë§ˆì¼€íŒ…", "ê´‘ê³ ", "ì½˜í…ì¸ ë§ˆì¼€íŒ…", "ì¸í”Œë£¨ì–¸ì„œ"],
                "description": "ì°½ì—…, ë§ˆì¼€íŒ…, ì˜¨ë¼ì¸ ì‚¬ì—…",
                "color": "linear-gradient(135deg, #34495e, #2c3e50)"
            },
            "ğŸ  ë¼ì´í”„ìŠ¤íƒ€ì¼": {
                "keywords": ["ì¸í…Œë¦¬ì–´", "ì •ë¦¬ì •ëˆ", "ë¯¸ë‹ˆë©€ë¼ì´í”„", "ê°€ë“œë‹", "ë°˜ë ¤ë™ë¬¼", "ìœ¡ì•„", "êµìœ¡", "íŒ¨ì…˜", "ë·°í‹°", "ìŠ¤í‚¨ì¼€ì–´", "í™ˆë°ì½”", "ì‚´ë¦¼", "ì²­ì†Œ", "ìˆ˜ë‚©", "í™ˆì¹´í˜", "í”Œëœí…Œë¦¬ì–´"],
                "description": "ì¸í…Œë¦¬ì–´, íŒ¨ì…˜, ì¼ìƒ",
                "color": "linear-gradient(135deg, #1abc9c, #16a085)"
            },
            "ğŸ’» IT/í…Œí¬": {
                "keywords": ["í”„ë¡œê·¸ë˜ë°", "ì•±ê°œë°œ", "ì›¹ê°œë°œ", "ì½”ë”©", "ê°œë°œì", "ITíŠ¸ë Œë“œ", "ìŠ¤ë§ˆíŠ¸í°", "ê°¤ëŸ­ì‹œ", "ì•„ì´í°", "ë…¸íŠ¸ë¶", "íƒœë¸”ë¦¿", "AI", "ë¹…ë°ì´í„°", "í´ë¼ìš°ë“œ", "ë³´ì•ˆ", "ë¸”ë¡ì²´ì¸"],
                "description": "í”„ë¡œê·¸ë˜ë°, IT, í…Œí¬ ì •ë³´",
                "color": "linear-gradient(135deg, #6c5ce7, #5f3dc4)"
            },
            "ğŸ“š êµìœ¡/í•™ìŠµ": {
                "keywords": ["ì˜ì–´ê³µë¶€", "í† ìµ", "í† í”Œ", "ìê²©ì¦", "ê³µë¬´ì›", "ì·¨ì—…", "ë©´ì ‘", "ìê¸°ê³„ë°œ", "ì˜¨ë¼ì¸ê°•ì˜", "ë…í•™", "ìŠ¤í„°ë””", "ì‹œí—˜", "í•™ì›", "ì¸ê°•", "ì–´í•™ì—°ìˆ˜", "ìœ í•™"],
                "description": "êµìœ¡, í•™ìŠµ, ìê¸°ê³„ë°œ",
                "color": "linear-gradient(135deg, #fd79a8, #e84393)"
            },
            "ğŸš— ìë™ì°¨/ëª¨ë¹Œë¦¬í‹°": {
                "keywords": ["ìë™ì°¨", "ì „ê¸°ì°¨", "í•˜ì´ë¸Œë¦¬ë“œ", "ì¤‘ê³ ì°¨", "ì‹ ì°¨", "ìë™ì°¨ë¦¬ë·°", "ì¹´í˜", "ë“œë¼ì´ë¸Œ", "ìº í•‘ì¹´", "ëª¨í„°ì‚¬ì´í´", "ìì „ê±°", "í‚¥ë³´ë“œ", "ëŒ€ì¤‘êµí†µ", "íƒì‹œ", "ì¹´ì…°ì–´ë§", "ë Œí„°ì¹´"],
                "description": "ìë™ì°¨, ëª¨ë¹Œë¦¬í‹°, êµí†µ",
                "color": "linear-gradient(135deg, #00b894, #00a085)"
            },
            "ğŸ¥ ì˜ë£Œ/ê±´ê°•ì •ë³´": {
                "keywords": ["ê±´ê°•ê²€ì§„", "ë³‘ì›", "ì˜ë£Œì •ë³´", "ê±´ê°•ê´€ë¦¬", "ì•½ë¬¼", "ì§ˆë³‘", "ì˜ˆë°©ì ‘ì¢…", "í•œì˜í•™", "ì¹˜ê³¼", "ì„±í˜•", "í”¼ë¶€ê³¼", "ë‚´ê³¼", "ì •ì‹ ê±´ê°•", "ìŠ¤íŠ¸ë ˆìŠ¤", "ìš°ìš¸ì¦", "ë¶ˆì•ˆ"],
                "description": "ì˜ë£Œ, ê±´ê°•, ì§ˆë³‘ ì •ë³´",
                "color": "linear-gradient(135deg, #00cec9, #00b894)"
            }
        }

# ì›¹ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
blog_app = BlogWebApp()

def require_auth(f):
    """ì¸ì¦ í•„ìš” ë°ì½”ë ˆì´í„°"""
    from functools import wraps
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not session.get('authenticated'):
            return jsonify({'error': 'ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.', 'redirect': '/login'}), 401
        return f(*args, **kwargs)
    return decorated_function

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    # íŒ¨ìŠ¤ì›Œë“œ ì¸ì¦ ì²´í¬
    if not session.get('authenticated'):
        return redirect(url_for('login'))
    return render_template('index.html')

@app.route('/login', methods=['GET', 'POST'])
def login():
    """ë¡œê·¸ì¸ í˜ì´ì§€"""
    if request.method == 'POST':
        password = request.form.get('password')
        if password == ADMIN_PASSWORD:
            session['authenticated'] = True
            return redirect(url_for('index'))
        else:
            error = "ì˜ëª»ëœ íŒ¨ìŠ¤ì›Œë“œì…ë‹ˆë‹¤."
            return render_template('login.html', error=error)
    return render_template('login.html')

@app.route('/logout')
def logout():
    """ë¡œê·¸ì•„ì›ƒ"""
    session.pop('authenticated', None)
    return redirect(url_for('login'))

@app.route('/attached_assets/<path:filename>')
def attached_assets(filename):
    """attached_assets í´ë”ì˜ íŒŒì¼ ì„œë¹™"""
    return send_from_directory('attached_assets', filename)

@app.route('/api/search', methods=['POST'])
@require_auth
def api_search():
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API"""
    try:
        data = request.get_json()
        keyword = data.get('keyword', '').strip()
        search_count = data.get('search_count', 50)
        sort_type = data.get('sort_type', 'date')

        print(f"ğŸ” ê²€ìƒ‰ ìš”ì²­ ë°›ìŒ: '{keyword}' (ê°œìˆ˜: {search_count}, ì •ë ¬: {sort_type})")

        if not keyword:
            return jsonify({'error': 'í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”'}), 400

        # í™˜ê²½ë³€ìˆ˜ ì²´í¬
        if not blog_app.client_id or not blog_app.client_secret:
            return jsonify({
                'error': 'ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Secretsì—ì„œ NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRET_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.',
                'details': {
                    'client_id_set': bool(blog_app.client_id),
                    'client_secret_set': bool(blog_app.client_secret)
                }
            }), 400

        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰
        search_result = blog_app.search_naver_blog(keyword, search_count, sort_type)
        
        if not search_result:
            return jsonify({'error': 'ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 500
            
        titles, descriptions = blog_app.extract_blog_data(search_result)

        if not titles:
            return jsonify({'error': 'ê²€ìƒ‰ëœ ë¸”ë¡œê·¸ ì œëª©ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.'}), 404

        # ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
        session_id = str(uuid.uuid4())
        blog_app.temp_results[session_id] = {
            'keyword': keyword,
            'search_result': search_result,
            'titles': titles,
            'descriptions': descriptions,
            'timestamp': datetime.now()
        }

        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(titles)}ê°œ ì œëª© ìˆ˜ì§‘ë¨")

        return jsonify({
            'success': True,
            'session_id': session_id,
            'total_results': search_result.get('total', 0),
            'collected_titles': len(titles),
            'titles': titles,  # ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜
            'average_length': sum(len(t) for t in titles)/len(titles) if titles else 0
        })

    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ API ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"ğŸ“ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        
        return jsonify({
            'error': str(e),
            'type': 'search_error',
            'suggestion': 'ë„¤ì´ë²„ API í‚¤ ì„¤ì •ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.'
        }), 500

@app.route('/api/analyze', methods=['POST'])
@require_auth
def api_analyze():
    """AI ë¶„ì„ API"""
    try:
        data = request.get_json()
        session_id = data.get('session_id')
        analysis_type = data.get('analysis_type', 'comprehensive')

        if not session_id or session_id not in blog_app.temp_results:
            return jsonify({'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ì…ë‹ˆë‹¤'}), 400

        result_data = blog_app.temp_results[session_id]

        # AI ë¶„ì„ ì‹¤í–‰
        analysis_result = blog_app.analyze_with_gpt(
            result_data['titles'], 
            result_data['descriptions'], 
            result_data['keyword'], 
            analysis_type
        )

        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        blog_app.temp_results[session_id]['analysis_result'] = analysis_result
        blog_app.temp_results[session_id]['analysis_type'] = analysis_type

        return jsonify({
            'success': True,
            'analysis_result': analysis_result
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/generate_titles', methods=['POST'])
@require_auth
def api_generate_titles():
    """ìƒˆë¡œìš´ ì œëª© ìƒì„± API"""
    try:
        data = request.get_json()
        session_id = data.get('session_id')
        num_titles = data.get('num_titles', 10)

        if not session_id or session_id not in blog_app.temp_results:
            return jsonify({'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ì…ë‹ˆë‹¤'}), 400

        result_data = blog_app.temp_results[session_id]

        if 'analysis_result' not in result_data:
            return jsonify({'error': 'ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”'}), 400

        # ìƒˆë¡œìš´ ì œëª© ìƒì„±
        generated_titles = blog_app.generate_titles_with_gpt(
            result_data['analysis_result'],
            result_data['keyword'],
            num_titles
        )

        # ì œëª© íŒŒì‹± - ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì œëª©ë§Œ ì¶”ì¶œ
        extracted_titles = []
        
        if generated_titles and generated_titles.strip():
            print(f"ğŸ” ì›ë³¸ GPT ì‘ë‹µ: {generated_titles[:500]}...")
            
            # ì¤„ë³„ë¡œ ë¶„ë¦¬
            lines = [line.strip() for line in generated_titles.split('\n') if line.strip()]
            
            import re
            for line in lines:
                # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì°¾ê¸° (1. ì œëª©, 1) ì œëª©, 1 ì œëª© ë“±)
                match = re.match(r'^\d+[\.\)\s]\s*(.+)', line)
                if match:
                    title = match.group(1).strip()
                    
                    # ë§ˆí¬ë‹¤ìš´ ì œê±°
                    title = re.sub(r'\*\*([^*]+)\*\*', r'\1', title)
                    title = re.sub(r'\*([^*]+)\*', r'\1', title)
                    
                    # ê¸ˆì§€ëœ í‚¤ì›Œë“œë“¤ì´ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
                    forbidden_keywords = ['ìœ í˜•:', 'íƒ€ê²Ÿ:', 'ëª©ì :', 'í‚¤ì›Œë“œ:', 'íŠ¹ì§•:', 'ì„¤ëª…:', 
                                        '**ìœ í˜•**', '**íƒ€ê²Ÿ**', '**ëª©ì **', '**í‚¤ì›Œë“œ**', '**íŠ¹ì§•**', '**ì„¤ëª…**']
                    
                    # ì œëª©ì— ê¸ˆì§€ëœ í‚¤ì›Œë“œê°€ ì—†ê³ , ì ì ˆí•œ ê¸¸ì´ì¸ ê²½ìš°ë§Œ ì¶”ê°€
                    if (not any(keyword in title for keyword in forbidden_keywords) and 
                        title and len(title) > 5 and len(title) < 200):
                        extracted_titles.append(title)
                        print(f"âœ… ì¶”ì¶œëœ ì œëª©: {title}")
                    else:
                        print(f"âŒ ì œì™¸ëœ ë¼ì¸: {title}")
        
        # ì¶”ì¶œëœ ì œëª©ì´ ë¶€ì¡±í•˜ë©´ fallback ì œëª© ìƒì„±
        if len(extracted_titles) < num_titles:
            needed = num_titles - len(extracted_titles)
            print(f"âš ï¸ ì œëª© ë¶€ì¡± ({len(extracted_titles)}/{num_titles}), fallback ì œëª© {needed}ê°œ ì¶”ê°€")
            
            fallback_titles = [
                f"{result_data['keyword']} ì™„ë²½ ê°€ì´ë“œ",
                f"{result_data['keyword']} ì´ˆë³´ìë¥¼ ìœ„í•œ ì•ˆë‚´ì„œ", 
                f"{result_data['keyword']} ì•Œì•„ì•¼ í•  ëª¨ë“  ê²ƒ",
                f"{result_data['keyword']} ì‹¤ì „ í™œìš©ë²•",
                f"{result_data['keyword']} ì¶”ì²œ ë° í›„ê¸°",
                f"{result_data['keyword']} ì„±ê³µ ì‚¬ë¡€ ë¶„ì„",
                f"{result_data['keyword']} ì „ë¬¸ê°€ ë…¸í•˜ìš°",
                f"{result_data['keyword']} ë‹¨ê³„ë³„ ë°©ë²•ë¡ ",
                f"{result_data['keyword']} íŠ¸ë Œë“œ ë¶„ì„",
                f"{result_data['keyword']} ì‹¤ë¬´ ì ìš©ê¸°",
                f"{result_data['keyword']} ë¹„êµ ë¶„ì„",
                f"{result_data['keyword']} ì„ íƒ ê°€ì´ë“œ"
            ]
            
            # ê¸°ì¡´ ì œëª©ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” fallback ì œëª©ë§Œ ì¶”ê°€
            for fallback in fallback_titles:
                if fallback not in extracted_titles and len(extracted_titles) < num_titles:
                    extracted_titles.append(fallback)

        # ìµœì¢… ì œëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ê°œìˆ˜ë¡œ ì œí•œ
        extracted_titles = extracted_titles[:num_titles]
        
        print(f"ğŸ¯ ìµœì¢… ì œëª© ê°œìˆ˜: {len(extracted_titles)}")
        for i, title in enumerate(extracted_titles):
            print(f"  {i+1}. {title}")

        # ìƒì„±ëœ ì œëª© ì €ì¥
        blog_app.temp_results[session_id]['generated_titles'] = extracted_titles

        return jsonify({
            'success': True,
            'titles': extracted_titles
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/generate_blog', methods=['POST'])
@require_auth
def api_generate_blog():
    """ë¸”ë¡œê·¸ ê¸€ ìƒì„± API"""
    try:
        data = request.get_json()
        session_id = data.get('session_id')
        selected_title = data.get('title')
        prompt_type = data.get('prompt_type', 'informative')
        additional_prompt = data.get('additional_prompt', '')
        min_chars = data.get('min_chars', 4000)
        max_chars = data.get('max_chars', 8000)

        if not session_id or session_id not in blog_app.temp_results:
            return jsonify({'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ì…ë‹ˆë‹¤'}), 400

        if not selected_title:
            return jsonify({'error': 'ì œëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš”'}), 400

        result_data = blog_app.temp_results[session_id]

        # ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ì „ë‹¬
        analysis_result = result_data.get('analysis_result')

        # ë¸”ë¡œê·¸ ê¸€ ìƒì„± (ê¸€ììˆ˜ ì„¤ì • ë° ë¶„ì„ ê²°ê³¼ í¬í•¨)
        blog_content = blog_app.generate_blog_content(
            selected_title,
            result_data['keyword'],
            prompt_type,
            additional_prompt,
            min_chars,
            max_chars,
            analysis_result
        )

        # ìƒì„±ëœ ë¸”ë¡œê·¸ ê¸€ ì €ì¥
        blog_app.temp_results[session_id]['blog_content'] = {
            'title': selected_title,
            'content': blog_content,
            'prompt_type': prompt_type,
            'additional_prompt': additional_prompt,
            'min_chars': min_chars,
            'max_chars': max_chars
        }

        # SEO ë¶„ì„ ìˆ˜í–‰
        seo_analysis = blog_app._analyze_seo_content(blog_content, result_data['keyword'])
        
        return jsonify({
            'success': True,
            'title': selected_title,
            'content': blog_content,
            'char_count': len(blog_content),
            'seo_analysis': seo_analysis,
            'keyword': result_data['keyword']
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/generate_images', methods=['POST'])
@require_auth
def api_generate_images():
    """ì´ë¯¸ì§€ ìƒì„± API"""
    try:
        data = request.get_json()
        session_id = data.get('session_id')
        num_images = data.get('num_images', 4)

        if not session_id or session_id not in blog_app.temp_results:
            return jsonify({'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ì…ë‹ˆë‹¤'}), 400

        result_data = blog_app.temp_results[session_id]

        if 'blog_content' not in result_data:
            return jsonify({'error': 'ë¨¼ì € ë¸”ë¡œê·¸ ê¸€ì„ ìƒì„±í•´ì£¼ì„¸ìš”'}), 400

        blog_data = result_data['blog_content']

        # ì´ë¯¸ì§€ ìƒì„±
        generated_images = blog_app.generate_dall_e_images(
            blog_data['title'],
            blog_data['content'],
            result_data['keyword'],
            num_images
        )

        # ìƒì„±ëœ ì´ë¯¸ì§€ ì €ì¥
        blog_app.temp_results[session_id]['generated_images'] = generated_images

        return jsonify({
            'success': True,
            'images': generated_images
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/categories')
@require_auth
def api_categories():
    """ì¹´í…Œê³ ë¦¬ ëª©ë¡ API"""
    try:
        categories = blog_app.get_predefined_categories()
        return jsonify({
            'success': True,
            'categories': categories
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/category-keywords')
@require_auth
def api_category_keywords():
    """ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ API"""
    try:
        categories = blog_app.get_predefined_categories()
        category_list = []
        
        for name, data in categories.items():
            category_list.append({
                'name': name,
                'icon': name.split()[0],
                'description': data['description'],
                'keywords': data['keywords']
            })
        
        return jsonify(category_list)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/recommended-keywords')
@require_auth
def api_recommended_keywords_route():
    """ì¶”ì²œ í‚¤ì›Œë“œ API"""
    try:
        # Google íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        google_keywords = blog_app.get_google_trending_keywords()
        
        # ë„¤ì´ë²„ DataLab íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°  
        naver_keywords = blog_app.get_naver_datalab_keywords()
        
        # ê²°ê³¼ ì¡°í•©
        trending_data = {
            'google_trends': google_keywords,
            'naver_datalab': naver_keywords,
            'update_time': datetime.now().isoformat()
        }
        
        return jsonify(trending_data)
        
    except Exception as e:
        print(f"íŠ¸ë Œë“œ í‚¤ì›Œë“œ API ì˜¤ë¥˜: {str(e)}")
        # Fallback ë°ì´í„° ë°˜í™˜
        fallback_data = {
            'google_trends': [
                {'rank': 1, 'keyword': 'ChatGPT', 'category': 'ğŸ¤– AI', 'source': 'Fallback'},
                {'rank': 2, 'keyword': 'ë‹¤ì´ì–´íŠ¸', 'category': 'ğŸ’ª ê±´ê°•', 'source': 'Fallback'}
            ],
            'naver_datalab': [
                {'rank': 1, 'keyword': 'ìš”ë¦¬ë ˆì‹œí”¼', 'category': 'ğŸ³ ìš”ë¦¬', 'source': 'Fallback'},
                {'rank': 2, 'keyword': 'í™ˆíŠ¸ë ˆì´ë‹', 'category': 'ğŸ’ª ìš´ë™', 'source': 'Fallback'}
            ],
            'update_time': datetime.now().isoformat()
        }
        return jsonify(fallback_data)

@app.route('/api/recommended_keywords')
def api_recommended_keywords():
    """ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ API (Google + Naver)"""
    try:
        import random
        from datetime import datetime
        
        current_hour = datetime.now().hour
        
        # Google íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        google_keywords = blog_app.get_google_trending_keywords()
        
        # ë„¤ì´ë²„ DataLab íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°  
        naver_keywords = blog_app.get_naver_datalab_keywords()
        
        # ê²°ê³¼ ì¡°í•©
        trending_data = {
            'google_trends': google_keywords,
            'naver_trends': naver_keywords,
            'time_period': f"ğŸ• {current_hour}ì‹œ",
            'update_time': datetime.now().isoformat()
        }
        
        return jsonify({
            'success': True,
            'trending_data': trending_data
        })
        
    except Exception as e:
        print(f"íŠ¸ë Œë“œ í‚¤ì›Œë“œ API ì˜¤ë¥˜: {str(e)}")
        # Fallbackìœ¼ë¡œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        return api_recommended_keywords_fallback()

def api_recommended_keywords_fallback():
    """ì¶”ì²œ í‚¤ì›Œë“œ API ì‹¤íŒ¨ì‹œ ëŒ€ì²´ í•¨ìˆ˜"""
    try:
        import random
        from datetime import datetime
        
        current_hour = datetime.now().hour
        
        # ì‹œê°„ëŒ€ë³„ í‚¤ì›Œë“œ í’€
        time_keywords = {
            'morning': [
                {'keyword': 'ëª¨ë‹ë£¨í‹´', 'category': 'ğŸŒ… ë¼ì´í”„'},
                {'keyword': 'ì•„ì¹¨ìš´ë™', 'category': 'ğŸŒ… ê±´ê°•'},
                {'keyword': 'ì•„ì¹¨ì‹ë‹¨', 'category': 'ğŸŒ… ìš”ë¦¬'}
            ],
            'afternoon': [
                {'keyword': 'ì ì‹¬ë©”ë‰´', 'category': 'â˜€ï¸ ìš”ë¦¬'},
                {'keyword': 'ì—…ë¬´íš¨ìœ¨', 'category': 'â˜€ï¸ ë¹„ì¦ˆë‹ˆìŠ¤'},
                {'keyword': 'ì¹´í˜ì¶”ì²œ', 'category': 'â˜€ï¸ ë§›ì§‘'}
            ],
            'evening': [
                {'keyword': 'ì €ë…ìš”ë¦¬', 'category': 'ğŸŒ™ ìš”ë¦¬'},
                {'keyword': 'ë„·í”Œë¦­ìŠ¤', 'category': 'ğŸŒ™ ì—”í„°'},
                {'keyword': 'ë…ì„œ', 'category': 'ğŸŒ™ ì·¨ë¯¸'}
            ]
        }
        
        # ì‹œê°„ëŒ€ë³„ ì„ íƒ
        if 6 <= current_hour < 12:
            selected = time_keywords['morning']
            time_label = "ğŸŒ… ì•„ì¹¨"
        elif 12 <= current_hour < 18:
            selected = time_keywords['afternoon'] 
            time_label = "â˜€ï¸ ì˜¤í›„"
        else:
            selected = time_keywords['evening']
            time_label = "ğŸŒ™ ì €ë…"
        
        # Fallback íŠ¸ë Œë“œ ë°ì´í„°
        trending_data = {
            'google_trends': [
                {'rank': 1, 'keyword': 'ChatGPT', 'category': 'ğŸ¤– AI', 'source': 'Fallback'},
                {'rank': 2, 'keyword': 'ë‹¤ì´ì–´íŠ¸', 'category': 'ğŸ’ª ê±´ê°•', 'source': 'Fallback'}
            ],
            'naver_trends': [
                {'rank': 1, 'keyword': 'ìš”ë¦¬ë ˆì‹œí”¼', 'category': 'ğŸ³ ìš”ë¦¬', 'source': 'Fallback'},
                {'rank': 2, 'keyword': 'í™ˆíŠ¸ë ˆì´ë‹', 'category': 'ğŸ’ª ìš´ë™', 'source': 'Fallback'}
            ],
            'time_period': time_label,
            'update_time': datetime.now().isoformat()
        }
        
        return jsonify({
            'success': True,
            'trending_data': trending_data
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/results/<session_id>')
def api_get_results(session_id):
    """ê²°ê³¼ ì¡°íšŒ API"""
    try:
        if session_id not in blog_app.temp_results:
            return jsonify({'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ì…ë‹ˆë‹¤'}), 400

        result_data = blog_app.temp_results[session_id]

        return jsonify({
            'success': True,
            'data': {
                'keyword': result_data.get('keyword'),
                'titles': result_data.get('titles', []),
                'analysis_result': result_data.get('analysis_result'),
                'generated_titles': result_data.get('generated_titles', []),
                'blog_content': result_data.get('blog_content'),
                'generated_images': result_data.get('generated_images', [])
            }
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/search', methods=['POST'])
def search():
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
    try:
        data = request.get_json()
        keyword = data.get('keyword', '').strip()
        display = data.get('display', 15)
        sort = data.get('sort', 'sim')

        if not keyword:
            return jsonify({'error': 'í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”'}), 400

        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰
        search_result = blog_app.search_naver_blog(keyword, display, sort)
        
        if not search_result or 'items' not in search_result:
            return jsonify({'error': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤'}), 400

        # ê²°ê³¼ ê°€ê³µ
        results = []
        for item in search_result['items']:
            results.append({
                'title': blog_app.clean_html_tags(item.get('title', '')),
                'description': blog_app.clean_html_tags(item.get('description', '')),
                'link': item.get('link', ''),
                'postdate': item.get('postdate', '')
            })

        # ì„¸ì…˜ì— ê²°ê³¼ ì €ì¥
        session_id = str(uuid.uuid4())
        blog_app.temp_results[session_id] = {
            'keyword': keyword,
            'search_result': search_result,
            'results': results,
            'timestamp': datetime.now()
        }

        return jsonify({
            'success': True,
            'keyword': keyword,
            'results': results,
            'session_id': session_id,
            'total': search_result.get('total', 0)
        })

    except Exception as e:
        print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/analyze', methods=['POST'])
def analyze():
    """AI ë¶„ì„"""
    try:
        # ì„¸ì…˜ì—ì„œ ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        latest_session = None
        for session_id, data in blog_app.temp_results.items():
            if latest_session is None or data['timestamp'] > blog_app.temp_results[latest_session]['timestamp']:
                latest_session = session_id

        if not latest_session:
            return jsonify({'error': 'ë¨¼ì € ê²€ìƒ‰ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”'}), 400

        result_data = blog_app.temp_results[latest_session]
        titles = [item['title'] for item in result_data['results']]
        descriptions = [item['description'] for item in result_data['results']]

        # AI ë¶„ì„ ì‹¤í–‰
        analysis_result = blog_app.analyze_with_gpt(
            titles, descriptions, result_data['keyword']
        )

        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        blog_app.temp_results[latest_session]['analysis_result'] = analysis_result

        return jsonify({
            'success': True,
            'analysis': analysis_result,
            'session_id': latest_session
        })

    except Exception as e:
        print(f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/generate-titles', methods=['POST'])
def generate_titles():
    """ì œëª© ìƒì„±"""
    try:
        # ì„¸ì…˜ì—ì„œ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        latest_session = None
        for session_id, data in blog_app.temp_results.items():
            if 'analysis_result' in data:
                if latest_session is None or data['timestamp'] > blog_app.temp_results[latest_session]['timestamp']:
                    latest_session = session_id

        if not latest_session:
            return jsonify({'error': 'ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”'}), 400

        result_data = blog_app.temp_results[latest_session]

        # ì œëª© ìƒì„±
        generated_titles = blog_app.generate_titles_with_gpt(
            result_data['analysis_result'],
            result_data['keyword'],
            10
        )

        # ì œëª© íŒŒì‹±
        titles = []
        if generated_titles:
            lines = generated_titles.split('\n')
            for line in lines:
                line = line.strip()
                if line and not line.startswith('#') and len(line) > 5:
                    # ë²ˆí˜¸ ì œê±°
                    import re
                    cleaned = re.sub(r'^\d+\.\s*', '', line)
                    if cleaned:
                        titles.append(cleaned)

        # ìƒì„±ëœ ì œëª© ì €ì¥
        blog_app.temp_results[latest_session]['generated_titles'] = titles

        return jsonify({
            'success': True,
            'titles': titles,
            'session_id': latest_session
        })

    except Exception as e:
        print(f"ì œëª© ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/generate-blog', methods=['POST'])
def generate_blog():
    """ë¸”ë¡œê·¸ ê¸€ ìƒì„±"""
    try:
        data = request.get_json()
        title = data.get('title', '').strip()
        min_chars = data.get('min_chars', 4000)
        max_chars = data.get('max_chars', 8000)

        if not title:
            return jsonify({'error': 'ì œëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš”'}), 400

        # ì„¸ì…˜ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
        keyword = None
        analysis_result = None
        for session_id, session_data in blog_app.temp_results.items():
            if 'keyword' in session_data:
                keyword = session_data['keyword']
                analysis_result = session_data.get('analysis_result')
                break

        if not keyword:
            return jsonify({'error': 'í‚¤ì›Œë“œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}), 400

        # ë¸”ë¡œê·¸ ê¸€ ìƒì„±
        blog_content = blog_app.generate_blog_content(
            title, keyword, 'informative', '', min_chars, max_chars, analysis_result
        )

        return jsonify({
            'success': True,
            'title': title,
            'content': blog_content,
            'char_count': len(blog_content)
        })

    except Exception as e:
        print(f"ë¸”ë¡œê·¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/generate-images', methods=['POST'])
def generate_images():
    """ì´ë¯¸ì§€ ìƒì„±"""
    try:
        data = request.get_json()
        title = data.get('title', '')
        content = data.get('content', '')
        count = data.get('count', 4)

        if not title or not content:
            return jsonify({'error': 'ì œëª©ê³¼ ë‚´ìš©ì´ í•„ìš”í•©ë‹ˆë‹¤'}), 400

        # í‚¤ì›Œë“œì™€ ì„¸ì…˜ ID ì°¾ê¸°
        keyword = None
        target_session_id = None
        for session_id, session_data in blog_app.temp_results.items():
            if 'keyword' in session_data:
                keyword = session_data['keyword']
                target_session_id = session_id
                break

        if not keyword:
            keyword = title  # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì œëª© ì‚¬ìš©

        # ì´ë¯¸ì§€ ìƒì„±
        generated_images = blog_app.generate_dall_e_images(
            title, content, keyword, count
        )

        # ì„¸ì…˜ì— ì´ë¯¸ì§€ ì €ì¥ (ì„¸ì…˜ IDê°€ ìˆëŠ” ê²½ìš°)
        if target_session_id and target_session_id in blog_app.temp_results:
            blog_app.temp_results[target_session_id]['generated_images'] = generated_images
            print(f"âœ… ì´ë¯¸ì§€ê°€ ì„¸ì…˜ {target_session_id}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return jsonify({
            'success': True,
            'images': generated_images,
            'session_id': target_session_id  # í´ë¼ì´ì–¸íŠ¸ì— ì„¸ì…˜ ID ì „ë‹¬
        })

    except Exception as e:
        print(f"ì´ë¯¸ì§€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/download_images/<session_id>')
def api_download_images(session_id):
    """ì´ë¯¸ì§€ ZIP ë‹¤ìš´ë¡œë“œ API"""
    try:
        print(f"ğŸ” ë‹¤ìš´ë¡œë“œ ìš”ì²­ ì„¸ì…˜ ID: {session_id}")
        print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ì…˜ë“¤: {list(blog_app.temp_results.keys())}")
        
        if session_id not in blog_app.temp_results:
            print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ ID: {session_id}")
            return jsonify({'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ì…ë‹ˆë‹¤'}), 400

        result_data = blog_app.temp_results[session_id]
        generated_images = result_data.get('generated_images', [])
        
        print(f"ğŸ“¸ ì°¾ì€ ì´ë¯¸ì§€ ê°œìˆ˜: {len(generated_images)}")

        if not generated_images:
            print("âŒ ë‹¤ìš´ë¡œë“œí•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return jsonify({'error': 'ë‹¤ìš´ë¡œë“œí•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.'}), 400

        import zipfile
        import io
        import urllib.request
        import urllib.error

        # ë©”ëª¨ë¦¬ì—ì„œ ZIP íŒŒì¼ ìƒì„±
        zip_buffer = io.BytesIO()
        successful_downloads = 0

        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for i, image in enumerate(generated_images):
                try:
                    print(f"ğŸ“¥ ì´ë¯¸ì§€ {i+1} ë‹¤ìš´ë¡œë“œ ì‹œì‘: {image['url'][:50]}...")
                    
                    # urllibì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (requests ëŒ€ì‹ )
                    request = urllib.request.Request(image['url'])
                    request.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
                    
                    with urllib.request.urlopen(request, timeout=30) as response:
                        image_data = response.read()

                    # ZIP íŒŒì¼ì— ì´ë¯¸ì§€ ì¶”ê°€
                    filename = f"generated_image_{i+1}.png"
                    zip_file.writestr(filename, image_data)
                    successful_downloads += 1
                    
                    print(f"âœ… ì´ë¯¸ì§€ {i+1} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

                except Exception as e:
                    print(f"âŒ ì´ë¯¸ì§€ {i+1} ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
                    continue

        if successful_downloads == 0:
            return jsonify({'error': 'ëª¨ë“  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'}), 500

        zip_buffer.seek(0)

        # í‚¤ì›Œë“œë‚˜ ì œëª©ì„ ì‚¬ìš©í•œ íŒŒì¼ëª… ìƒì„±
        keyword = result_data.get('keyword', 'images')
        safe_keyword = "".join(c for c in keyword if c.isalnum() or c in (' ', '-', '_')).rstrip()
        if not safe_keyword:
            safe_keyword = 'generated_images'
        filename = f"{safe_keyword}_images.zip"
        
        print(f"ğŸ“¦ ZIP íŒŒì¼ ìƒì„± ì™„ë£Œ: {filename} ({successful_downloads}ê°œ ì´ë¯¸ì§€)")

        return send_file(
            zip_buffer,
            mimetype='application/zip',
            as_attachment=True,
            download_name=filename
        )

    except Exception as e:
        print(f"âŒ ZIP íŒŒì¼ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"ğŸ“ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return jsonify({'error': f'ZIP íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}), 500

if __name__ == '__main__':
    import os
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_ENV') != 'production'
    app.run(debug=debug, host='0.0.0.0', port=port)